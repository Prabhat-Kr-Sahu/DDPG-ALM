{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpG5tjpFBI-C"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv3IDvrobU37"
      },
      "source": [
        "# Deep Reinforcement Learning for Stock Trading from Scratch: Portfolio Allocation\n",
        "\n",
        "Tutorials to use OpenAI DRL to perform portfolio allocation in one Jupyter Notebook | Presented at NeurIPS 2020: Deep RL Workshop\n",
        "\n",
        "* This blog is based on our paper: FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance, presented at NeurIPS 2020: Deep RL Workshop.\n",
        "* Check out medium blog for detailed explanations: https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-portfolio-allocation-9b417660c7cd\n",
        "* Please report any issues to our Github: https://github.com/AI4Finance-Foundation/FinRL/issues\n",
        "\n",
        "ESG-VARIABLES-PENALIZING\n",
        "* **Pytorch Version**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kHCfEiTA80V"
      },
      "source": [
        "# Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUmLTmoQA7_w"
      },
      "source": [
        "* [1. Problem Definition](#0)\n",
        "* [2. Getting Started - Load Python packages](#1)\n",
        "    * [2.1. Install Packages](#1.1)    \n",
        "    * [2.2. Check Additional Packages](#1.2)\n",
        "    * [2.3. Import Packages](#1.3)\n",
        "    * [2.4. Create Folders](#1.4)\n",
        "* [3. Download Data](#2)\n",
        "* [4. Preprocess Data](#3)        \n",
        "    * [4.1. Technical Indicators](#3.1)\n",
        "    * [4.2. Perform Feature Engineering](#3.2)\n",
        "* [5.Build Environment](#4)  \n",
        "    * [5.1. Training & Trade Data Split](#4.1)\n",
        "    * [5.2. User-defined Environment](#4.2)   \n",
        "    * [5.3. Initialize Environment](#4.3)    \n",
        "* [6.Implement DRL Algorithms](#5)  \n",
        "* [7.Backtesting Performance](#6)  \n",
        "    * [7.1. BackTestStats](#6.1)\n",
        "    * [7.2. BackTestPlot](#6.2)   \n",
        "    * [7.3. Baseline Stats](#6.3)   \n",
        "    * [7.3. Compare to Stock Market Index](#6.4)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf_S-hi4BVx6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12v1i0jVkg48"
      },
      "source": [
        "<a id='0'></a>\n",
        "# Part 1. Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2DXqvLpBZkc"
      },
      "source": [
        "# **Problem Definition: Risk-Constrained Portfolio Optimization Using Deep Reinforcement Learning**  \n",
        "\n",
        "This problem aims to design an **automated trading solution** for portfolio allocation while ensuring risk constraints are satisfied.  \n",
        "We model the stock trading process as a **Constrained Markov Decision Process (CMDP)** and formulate our objective as a **constrained maximization problem**:  \n",
        "- **Maximize** portfolio returns.  \n",
        "- **Minimize** risk exposure within predefined limits.  \n",
        "\n",
        "The algorithm is trained using **Deep Reinforcement Learning (DRL)** techniques, integrating **Deep Deterministic Policy Gradient (DDPG) with an Augmented Lagrangian Multiplier (ALM)** to handle risk constraints dynamically.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Reinforcement Learning Environment Components**  \n",
        "\n",
        "### **1️⃣ Action Space**  \n",
        "- The **agent selects portfolio weights** for each asset at each time step.  \n",
        "- Action vector:  \n",
        "  \\[\n",
        "  \\mathbf{a} = [a_1, a_2, \\dots, a_N], \\quad \\text{where } a_i \\in (-1,1) \\text{ and } \\sum_{i=1}^{N} a_i = 1\n",
        "  \\]\n",
        "- **Intuition:**  \n",
        "  - \\( a_i > 0 \\) → **Long** position in stock \\( i \\).  \n",
        "  - \\( a_i < 0 \\) → **Short** position in stock \\( i \\).  \n",
        "  - \\( a_i = 0 \\) → No investment in stock \\( i \\).  \n",
        "- Example: *\"Allocate 10% of capital to AAPL\"* → **Action = [0.1, ..., 0]**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2️⃣ State Space**  \n",
        "The agent **observes market conditions** before making trading decisions. The state vector contains:  \n",
        "- **Price-based features**: Open, High, Low, Close, Volume (OHLCV).  \n",
        "- **Technical indicators**: Moving Averages, RSI, MACD, Bollinger Bands.  \n",
        "- **Risk metrics**: Portfolio variance, Value-at-Risk (VaR).  \n",
        "- **Portfolio state**: Previous allocations, returns.  \n",
        "\n",
        "\\[\n",
        "\\mathbf{s} = [\\text{OHLCV}, \\text{indicators}, \\text{portfolio state}]\n",
        "\\]\n",
        "\n",
        "*Example:*  \n",
        "At time \\( t \\), the state could be:  \n",
        "\\[\n",
        "s_t = [\\text{AAPL close}, \\text{GOOGL RSI}, \\text{Portfolio return}, \\dots]\n",
        "\\]  \n",
        "\n",
        "---\n",
        "\n",
        "### **3️⃣ Reward Function**  \n",
        "The agent receives a reward based on portfolio performance:  \n",
        "\\[\n",
        "r(s, a, s') = v' - v - \\lambda \\cdot \\text{Risk}\n",
        "\\]\n",
        "Where:  \n",
        "- \\( v' \\) and \\( v \\) → Portfolio values before and after action.  \n",
        "- \\( \\lambda \\) → Lagrangian multiplier for risk constraint.  \n",
        "- **Risk term**: VaR, variance, or drawdown penalty.  \n",
        "\n",
        "This encourages the agent to **maximize return while controlling risk**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **4️⃣ Cost Function (Risk Constraint)**\n",
        "To enforce safety, we introduce a **risk-based cost function**:  \n",
        "\\[\n",
        "c(s, a) = \\max( \\text{VaR} - \\text{Risk Threshold}, 0)\n",
        "\\]\n",
        "- If **risk exceeds** the threshold, a penalty is applied.  \n",
        "- Otherwise, the cost is **zero**.  \n",
        "- The **Augmented Lagrangian Multiplier (ALM)** updates dynamically to enforce this constraint.  \n",
        "\n",
        "---\n",
        "\n",
        "### **5️⃣ Environment**\n",
        "The trading environment consists of **S&P 500 stocks** (or another index).  \n",
        "- **Data Source**: Yahoo Finance API, Alpha Vantage, or Quandl.  \n",
        "- **Time Frame**: Daily, hourly, or minute-level data.  \n",
        "- **Stock Pool**: Top 50 stocks based on market cap.  \n",
        "\n",
        "Example:  \n",
        "- **Dow 30 Constituents** (AAPL, MSFT, TSLA, etc.).  \n",
        "- Data includes **OHLCV + technical indicators**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Key Contributions of this Model**  \n",
        "✅ **Risk-aware DRL framework** for portfolio allocation.  \n",
        "✅ **Handles risk dynamically** using CMDP and ALM.  \n",
        "✅ **Optimized for long-short portfolio strategies**.  \n",
        "✅ **Scalable to multiple assets and real-world trading.**  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp7XycNbCEps"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_emqQCCklVt"
      },
      "source": [
        "<a id='1'></a>\n",
        "# Part 2. Getting Started- Load Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "DZef6mehsKNB",
        "outputId": "fd3877f8-7476-4a61-f267-d1a660fe8cda"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'myenv (Python 3.12.10)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/Admin/DDPG-ALM/myenv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "pip install setuptools==66"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XslbvAMhDifI",
        "outputId": "aae8fa20-cd37-4cac-bbe8-b0f6ca442bb0"
      },
      "outputs": [],
      "source": [
        "!pip install stockstats\n",
        "!pip install hyperopt\n",
        "# !pip install pyfolio\n",
        "import stockstats\n",
        "from hyperopt import fmin, tpe, hp, Trials, space_eval\n",
        "# import pyfolio\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4QTDlFvDmgi"
      },
      "outputs": [],
      "source": [
        "\"\"\"Contains methods and classes to collect data from\n",
        "Yahoo Finance API\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "class YahooDownloader:\n",
        "    \"\"\"Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from neofinrl_config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from neofinrl_config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from neofinrl_config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_date: str, end_date: str, ticker_list: list):\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.ticker_list = ticker_list\n",
        "\n",
        "    def fetch_data(self, proxy=None, auto_adjust=False) -> pd.DataFrame:\n",
        "        \"\"\"Fetches data from Yahoo API\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        `pd.DataFrame`\n",
        "            7 columns: A date, open, high, low, close, volume and tick symbol\n",
        "            for the specified stock ticker\n",
        "        \"\"\"\n",
        "        # Download and save the data in a pandas DataFrame:\n",
        "        data_df = pd.DataFrame()\n",
        "        num_failures = 0\n",
        "        for tic in self.ticker_list:\n",
        "            temp_df = yf.download(\n",
        "                tic,\n",
        "                start=self.start_date,\n",
        "                end=self.end_date,\n",
        "                proxy=proxy,\n",
        "                auto_adjust=auto_adjust,\n",
        "            )\n",
        "            if temp_df.columns.nlevels != 1:\n",
        "                temp_df.columns = temp_df.columns.droplevel(1)\n",
        "            temp_df[\"tic\"] = tic\n",
        "            if len(temp_df) > 0:\n",
        "                # data_df = data_df.append(temp_df)\n",
        "                data_df = pd.concat([data_df, temp_df], axis=0)\n",
        "            else:\n",
        "                num_failures = num_failures+ 1\n",
        "        if num_failures == len(self.ticker_list):\n",
        "            raise ValueError(\"no data is fetched.\")\n",
        "        # reset the index, we want to use numbers as index instead of dates\n",
        "        data_df = data_df.reset_index()\n",
        "        try:\n",
        "            # convert the column names to standardized names\n",
        "            data_df.rename(\n",
        "                columns={\n",
        "                    \"Date\": \"date\",\n",
        "                    \"Adj Close\": \"adjcp\",\n",
        "                    \"Close\": \"close\",\n",
        "                    \"High\": \"high\",\n",
        "                    \"Low\": \"low\",\n",
        "                    \"Volume\": \"volume\",\n",
        "                    \"Open\": \"open\",\n",
        "                    \"tic\": \"tic\",\n",
        "                },\n",
        "                inplace=True,\n",
        "            )\n",
        "\n",
        "            # use adjusted close price instead of close price\n",
        "            data_df[\"close\"] = data_df[\"adjcp\"]\n",
        "            # drop the adjusted close price column\n",
        "            data_df = data_df.drop(labels=\"adjcp\", axis=1)\n",
        "        except NotImplementedError:\n",
        "            print(\"the features are not supported currently\")\n",
        "        # create day of the week column (monday = 0)\n",
        "        data_df[\"day\"] = data_df[\"date\"].dt.dayofweek\n",
        "        # convert date to standard string format, easy to filter\n",
        "        data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
        "        # drop missing data\n",
        "        data_df = data_df.dropna()\n",
        "        data_df = data_df.reset_index(drop=True)\n",
        "        print(\"Shape of DataFrame: \", data_df.shape)\n",
        "        # print(\"Display DataFrame: \", data_df.head())\n",
        "\n",
        "        data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
        "\n",
        "        return data_df\n",
        "\n",
        "    def select_equal_rows_stock(self, df):\n",
        "        df_check = df.tic.value_counts()\n",
        "        df_check = pd.DataFrame(df_check).reset_index()\n",
        "        df_check.columns = [\"tic\", \"counts\"]\n",
        "        mean_df = df_check.counts.mean()\n",
        "        equal_list = list(df.tic.value_counts() >= mean_df)\n",
        "        names = df.tic.value_counts().index\n",
        "        select_stocks_list = list(names[equal_list])\n",
        "        df = df[df.tic.isin(select_stocks_list)]\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fixp-2X1DpQ2"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from multiprocessing.sharedctypes import Value\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from stockstats import StockDataFrame as Sdf\n",
        "\n",
        "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    load csv dataset from path\n",
        "    :return: (df) pandas dataframe\n",
        "    \"\"\"\n",
        "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
        "    _data = pd.read_csv(file_name)\n",
        "    return _data\n",
        "\n",
        "\n",
        "def data_split(df, start, end, target_date_col=\"date\"):\n",
        "    \"\"\"\n",
        "    split the dataset into training or testing using date\n",
        "    :param data: (df) pandas dataframe, start, end\n",
        "    :return: (df) pandas dataframe\n",
        "    \"\"\"\n",
        "    data = df[(df[target_date_col] >= start) & (df[target_date_col] < end)]\n",
        "    data = data.sort_values([target_date_col, \"tic\"], ignore_index=True)\n",
        "    data.index = data[target_date_col].factorize()[0]\n",
        "    return data\n",
        "\n",
        "\n",
        "def convert_to_datetime(time):\n",
        "    time_fmt = \"%Y-%m-%dT%H:%M:%S\"\n",
        "    if isinstance(time, str):\n",
        "        return datetime.datetime.strptime(time, time_fmt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk1PkjmeDt-Y"
      },
      "outputs": [],
      "source": [
        "# from __future__ import annotations\n",
        "\n",
        "# import copy\n",
        "# import datetime\n",
        "# from copy import deepcopy\n",
        "\n",
        "# !pip install empyrical\n",
        "# import empyrical as ep\n",
        "\n",
        "# import matplotlib.dates as mdates\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# !pip install pyfolio\n",
        "# import pyfolio\n",
        "# from pyfolio import timeseries\n",
        "# import itertools\n",
        "\n",
        "# # Replacing from pyfolio import timeseries with original codes ##\n",
        "\n",
        "# def gross_lev(positions):\n",
        "#     \"\"\"\n",
        "#     Calculates the gross leverage of a strategy.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     positions : pd.DataFrame\n",
        "#         Daily net position values.\n",
        "#          - See full explanation in tears.create_full_tear_sheet.\n",
        "\n",
        "#     Returns\n",
        "#     -------\n",
        "#     pd.Series\n",
        "#         Gross leverage.\n",
        "#     \"\"\"\n",
        "\n",
        "#     exposure = positions.drop('cash', axis=1).abs().sum(axis=1)\n",
        "#     return exposure / positions.sum(axis=1)\n",
        "\n",
        "# def get_txn_vol(transactions):\n",
        "#     \"\"\"\n",
        "#     Extract daily transaction data from set of transaction objects.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     transactions : pd.DataFrame\n",
        "#         Time series containing one row per symbol (and potentially\n",
        "#         duplicate datetime indices) and columns for amount and\n",
        "#         price.\n",
        "\n",
        "#     Returns\n",
        "#     -------\n",
        "#     pd.DataFrame\n",
        "#         Daily transaction volume and number of shares.\n",
        "#          - See full explanation in tears.create_full_tear_sheet.\n",
        "#     \"\"\"\n",
        "\n",
        "#     txn_norm = transactions.copy()\n",
        "#     txn_norm.index = txn_norm.index.normalize()\n",
        "#     amounts = txn_norm.amount.abs()\n",
        "#     prices = txn_norm.price\n",
        "#     values = amounts * prices\n",
        "#     daily_amounts = amounts.groupby(amounts.index).sum()\n",
        "#     daily_values = values.groupby(values.index).sum()\n",
        "#     daily_amounts.name = \"txn_shares\"\n",
        "#     daily_values.name = \"txn_volume\"\n",
        "#     return pd.concat([daily_values, daily_amounts], axis=1)\n",
        "\n",
        "# def get_turnover(positions, transactions, denominator='AGB'):\n",
        "#     \"\"\"\n",
        "#      - Value of purchases and sales divided\n",
        "#     by either the actual gross book or the portfolio value\n",
        "#     for the time step.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     positions : pd.DataFrame\n",
        "#         Contains daily position values including cash.\n",
        "#         - See full explanation in tears.create_full_tear_sheet\n",
        "#     transactions : pd.DataFrame\n",
        "#         Prices and amounts of executed trades. One row per trade.\n",
        "#         - See full explanation in tears.create_full_tear_sheet\n",
        "#     denominator : str, optional\n",
        "#         Either 'AGB' or 'portfolio_value', default AGB.\n",
        "#         - AGB (Actual gross book) is the gross market\n",
        "#         value (GMV) of the specific algo being analyzed.\n",
        "#         Swapping out an entire portfolio of stocks for\n",
        "#         another will yield 200% turnover, not 100%, since\n",
        "#         transactions are being made for both sides.\n",
        "#         - We use average of the previous and the current end-of-period\n",
        "#         AGB to avoid singularities when trading only into or\n",
        "#         out of an entire book in one trading period.\n",
        "#         - portfolio_value is the total value of the algo's\n",
        "#         positions end-of-period, including cash.\n",
        "\n",
        "#     Returns\n",
        "#     -------\n",
        "#     turnover_rate : pd.Series\n",
        "#         timeseries of portfolio turnover rates.\n",
        "#     \"\"\"\n",
        "\n",
        "#     txn_vol = get_txn_vol(transactions)\n",
        "#     traded_value = txn_vol.txn_volume\n",
        "\n",
        "#     if denominator == 'AGB':\n",
        "#         # Actual gross book is the same thing as the algo's GMV\n",
        "#         # We want our denom to be avg(AGB previous, AGB current)\n",
        "#         AGB = positions.drop('cash', axis=1).abs().sum(axis=1)\n",
        "#         denom = AGB.rolling(2).mean()\n",
        "\n",
        "#         # Since the first value of pd.rolling returns NaN, we\n",
        "#         # set our \"day 0\" AGB to 0.\n",
        "#         denom.iloc[0] = AGB.iloc[0] / 2\n",
        "#     elif denominator == 'portfolio_value':\n",
        "#         denom = positions.sum(axis=1)\n",
        "#     else:\n",
        "#         raise ValueError(\n",
        "#             \"Unexpected value for denominator '{}'. The \"\n",
        "#             \"denominator parameter must be either 'AGB'\"\n",
        "#             \" or 'portfolio_value'.\".format(denominator)\n",
        "#         )\n",
        "\n",
        "#     denom.index = denom.index.normalize()\n",
        "#     turnover = traded_value.div(denom, axis='index')\n",
        "#     turnover = turnover.fillna(0)\n",
        "#     return turnover\n",
        "\n",
        "# SIMPLE_STAT_FUNCS = [\n",
        "#     ep.annual_return,\n",
        "#     ep.cum_returns_final,\n",
        "#     ep.annual_volatility,\n",
        "#     ep.sharpe_ratio,\n",
        "#     ep.calmar_ratio,\n",
        "#     ep.stability_of_timeseries,\n",
        "#     # ep.max_drawdown,\n",
        "#     ep.omega_ratio,\n",
        "#     # ep.sortino_ratio,\n",
        "#     # stats.skew,\n",
        "#     # stats.kurtosis,\n",
        "#     # ep.tail_ratio,\n",
        "#     # value_at_risk\n",
        "# ]\n",
        "\n",
        "# FACTOR_STAT_FUNCS = [\n",
        "#     # ep.alpha,\n",
        "#     # ep.beta,\n",
        "# ]\n",
        "\n",
        "# STAT_FUNC_NAMES = {\n",
        "#     'annual_return': 'Annual return',\n",
        "#     'cum_returns_final': 'Cumulative returns',\n",
        "#     'annual_volatility': 'Annual volatility',\n",
        "#     'sharpe_ratio': 'Sharpe ratio',\n",
        "#     'calmar_ratio': 'Calmar ratio',\n",
        "#     'stability_of_timeseries': 'Stability',\n",
        "#     # 'max_drawdown': 'Max drawdown',\n",
        "#     'omega_ratio': 'Omega ratio',\n",
        "#     # 'sortino_ratio': 'Sortino ratio',\n",
        "#     # 'skew': 'Skew',\n",
        "#     # 'kurtosis': 'Kurtosis',\n",
        "#     # 'tail_ratio': 'Tail ratio',\n",
        "#     # 'common_sense_ratio': 'Common sense ratio',\n",
        "#     # 'value_at_risk': 'Daily value at risk',\n",
        "#     # 'alpha': 'Alpha',\n",
        "#     # 'beta': 'Beta',\n",
        "# }\n",
        "\n",
        "\n",
        "# def perf_stats(returns, factor_returns=None, positions=None,\n",
        "#                transactions=None, turnover_denom='AGB'):\n",
        "#     \"\"\"\n",
        "#     Calculates various performance metrics of a strategy, for use in\n",
        "#     plotting.show_perf_stats.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     returns : pd.Series\n",
        "#         Daily returns of the strategy, noncumulative.\n",
        "#          - See full explanation in tears.create_full_tear_sheet.\n",
        "#     factor_returns : pd.Series, optional\n",
        "#         Daily noncumulative returns of the benchmark factor to which betas are\n",
        "#         computed. Usually a benchmark such as market returns.\n",
        "#          - This is in the same style as returns.\n",
        "#          - If None, do not compute alpha, beta, and information ratio.\n",
        "#     positions : pd.DataFrame\n",
        "#         Daily net position values.\n",
        "#          - See full explanation in tears.create_full_tear_sheet.\n",
        "#     transactions : pd.DataFrame\n",
        "#         Prices and amounts of executed trades. One row per trade.\n",
        "#         - See full explanation in tears.create_full_tear_sheet.\n",
        "#     turnover_denom : str\n",
        "#         Either AGB or portfolio_value, default AGB.\n",
        "#         - See full explanation in txn.get_turnover.\n",
        "\n",
        "#     Returns\n",
        "#     -------\n",
        "#     pd.Series\n",
        "#         Performance metrics.\n",
        "#     \"\"\"\n",
        "\n",
        "#     stats = pd.Series()\n",
        "#     for stat_func in SIMPLE_STAT_FUNCS:\n",
        "#         stats[STAT_FUNC_NAMES[stat_func.__name__]] = stat_func(returns)\n",
        "\n",
        "#     if positions is not None:\n",
        "#         stats['Gross leverage'] = gross_lev(positions).mean()\n",
        "#         if transactions is not None:\n",
        "#             stats['Daily turnover'] = get_turnover(positions,\n",
        "#                                                    transactions,\n",
        "#                                                    turnover_denom).mean()\n",
        "#     if factor_returns is not None:\n",
        "#         for stat_func in FACTOR_STAT_FUNCS:\n",
        "#             res = stat_func(returns, factor_returns)\n",
        "#             stats[STAT_FUNC_NAMES[stat_func.__name__]] = res\n",
        "\n",
        "#     return stats\n",
        "# #######################\n",
        "# def date2str(dat: datetime.date) -> str:\n",
        "#     return datetime.date.strftime(dat, \"%Y-%m-%d\")\n",
        "\n",
        "# def str2date(dat: str) -> datetime.date:\n",
        "#     return datetime.datetime.strptime(dat, \"%Y-%m-%d\").date()\n",
        "\n",
        "# def get_daily_return(df, value_col_name=\"account_value\"):\n",
        "#     df = deepcopy(df)\n",
        "#     df[\"daily_return\"] = df[value_col_name].pct_change(1)\n",
        "#     df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "#     df.set_index(\"date\", inplace=True, drop=True)\n",
        "#     df.index = df.index.tz_localize(\"UTC\")\n",
        "#     return pd.Series(df[\"daily_return\"], index=df.index)\n",
        "\n",
        "\n",
        "# def convert_daily_return_to_pyfolio_ts(df):\n",
        "#     strategy_ret = df.copy()\n",
        "#     strategy_ret[\"date\"] = pd.to_datetime(strategy_ret[\"date\"])\n",
        "#     strategy_ret.set_index(\"date\", drop=False, inplace=True)\n",
        "#     strategy_ret.index = strategy_ret.index.tz_localize(\"UTC\")\n",
        "#     del strategy_ret[\"date\"]\n",
        "#     return pd.Series(strategy_ret[\"daily_return\"].values, index=strategy_ret.index)\n",
        "\n",
        "\n",
        "# # def backtest_stats(account_value, value_col_name=\"account_value\"):\n",
        "# #     dr_test = get_daily_return(account_value, value_col_name=value_col_name)\n",
        "# #     perf_stats_all = timeseries.perf_stats(\n",
        "# #         returns=dr_test,\n",
        "# #         positions=None,\n",
        "# #         transactions=None,\n",
        "# #         turnover_denom=\"AGB\",\n",
        "# #     )\n",
        "# #     print(perf_stats_all)\n",
        "# #     return perf_stats_all\n",
        "\n",
        "# def backtest_stats(account_value, value_col_name=\"account_value\"):\n",
        "#     dr_test = get_daily_return(account_value, value_col_name=value_col_name)\n",
        "#     perf_stats_all = perf_stats(\n",
        "#         returns=dr_test,\n",
        "#         positions=None,\n",
        "#         transactions=None,\n",
        "#         turnover_denom=\"AGB\",\n",
        "#     )\n",
        "#     print(perf_stats_all)\n",
        "#     return perf_stats_all\n",
        "\n",
        "\n",
        "# # def backtest_plot(\n",
        "# #     account_value,\n",
        "# #     baseline_start=TRADE_START_DATE,\n",
        "# #     baseline_end=TRADE_END_DATE,\n",
        "# #     baseline_ticker=\"^DJI\",\n",
        "# #     value_col_name=\"account_value\",\n",
        "# # ):\n",
        "# #     df = deepcopy(account_value)\n",
        "# #     df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "# #     test_returns = get_daily_return(df, value_col_name=value_col_name)\n",
        "\n",
        "# #     baseline_df = get_baseline(\n",
        "# #         ticker=baseline_ticker, start=baseline_start, end=baseline_end\n",
        "# #     )\n",
        "\n",
        "# #     baseline_df[\"date\"] = pd.to_datetime(baseline_df[\"date\"], format=\"%Y-%m-%d\")\n",
        "# #     baseline_df = pd.merge(df[[\"date\"]], baseline_df, how=\"left\", on=\"date\")\n",
        "# #     baseline_df = baseline_df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "# #     baseline_returns = get_daily_return(baseline_df, value_col_name=\"close\")\n",
        "\n",
        "# #     with pyfolio.plotting.plotting_context(font_scale=1.1):\n",
        "# #         pyfolio.create_full_tear_sheet(\n",
        "# #             returns=test_returns, benchmark_rets=baseline_returns, set_context=False\n",
        "# #         )\n",
        "\n",
        "\n",
        "# def get_baseline(ticker, start, end):\n",
        "#     return YahooDownloader(\n",
        "#         start_date=start, end_date=end, ticker_list=[ticker]\n",
        "#     ).fetch_data()\n",
        "\n",
        "\n",
        "# def trx_plot(df_trade, df_actions, ticker_list):\n",
        "#     df_trx = pd.DataFrame(np.array(df_actions[\"transactions\"].to_list()))\n",
        "#     df_trx.columns = ticker_list\n",
        "#     df_trx.index = df_actions[\"date\"]\n",
        "#     df_trx.index.name = \"\"\n",
        "\n",
        "#     for i in range(df_trx.shape[1]):\n",
        "#         df_trx_temp = df_trx.iloc[:, i]\n",
        "#         df_trx_temp_sign = np.sign(df_trx_temp)\n",
        "#         buying_signal = df_trx_temp_sign.apply(lambda x: x > 0)\n",
        "#         selling_signal = df_trx_temp_sign.apply(lambda x: x < 0)\n",
        "\n",
        "#         tic_plot = df_trade[\n",
        "#             (df_trade[\"tic\"] == df_trx_temp.name)\n",
        "#             & (df_trade[\"date\"].isin(df_trx.index))\n",
        "#         ][\"close\"]\n",
        "#         tic_plot.index = df_trx_temp.index\n",
        "\n",
        "#         plt.figure(figsize=(10, 8))\n",
        "#         plt.plot(tic_plot, color=\"g\", lw=2.0)\n",
        "#         plt.plot(\n",
        "#             tic_plot,\n",
        "#             \"^\",\n",
        "#             markersize=10,\n",
        "#             color=\"m\",\n",
        "#             label=\"buying signal\",\n",
        "#             markevery=buying_signal,\n",
        "#         )\n",
        "#         plt.plot(\n",
        "#             tic_plot,\n",
        "#             \"v\",\n",
        "#             markersize=10,\n",
        "#             color=\"k\",\n",
        "#             label=\"selling signal\",\n",
        "#             markevery=selling_signal,\n",
        "#         )\n",
        "#         plt.title(\n",
        "#             f\"{df_trx_temp.name} Num Transactions: {len(buying_signal[buying_signal == True]) + len(selling_signal[selling_signal == True])}\"\n",
        "#         )\n",
        "#         plt.legend()\n",
        "#         plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=25))\n",
        "#         plt.xticks(rotation=45, ha=\"right\")\n",
        "#         plt.show()\n",
        "\n",
        "\n",
        "# # 2022-01-15 -> 01/15/2022\n",
        "# def transfer_date(str_dat):\n",
        "#     return datetime.datetime.strptime(str_dat, \"%Y-%m-%d\").date().strftime(\"%m/%d/%Y\")\n",
        "\n",
        "\n",
        "# def plot_result_from_csv(\n",
        "#     csv_file: str,\n",
        "#     column_as_x: str,\n",
        "#     savefig_filename: str = \"fig/result.png\",\n",
        "#     xlabel: str = \"Date\",\n",
        "#     ylabel: str = \"Result\",\n",
        "#     num_days_xticks: int = 20,\n",
        "#     xrotation: int = 0,\n",
        "# ):\n",
        "#     result = pd.read_csv(csv_file)\n",
        "#     plot_result(\n",
        "#         result,\n",
        "#         column_as_x,\n",
        "#         savefig_filename,\n",
        "#         xlabel,\n",
        "#         ylabel,\n",
        "#         num_days_xticks,\n",
        "#         xrotation,\n",
        "#     )\n",
        "\n",
        "\n",
        "# # select_start_date: included\n",
        "# # select_end_date: included\n",
        "# # is if_need_calc_return is True, it is account_value, and then transfer it to return\n",
        "# # it is better that column_as_x is the first column, and the other columns are strategies\n",
        "# # xrotation: the rotation of xlabel, may be used in dates. Default=0 (adaptive adjustment)\n",
        "# def plot_result(\n",
        "#     result: pd.DataFrame(),\n",
        "#     column_as_x: str,\n",
        "#     savefig_filename: str = \"fig/result.png\",\n",
        "#     xlabel: str = \"Date\",\n",
        "#     ylabel: str = \"Result\",\n",
        "#     num_days_xticks: int = 20,\n",
        "#     xrotation: int = 0,\n",
        "# ):\n",
        "#     columns = result.columns\n",
        "#     columns_strtegy = []\n",
        "#     for i in range(len(columns)):\n",
        "#         col = columns[i]\n",
        "#         if \"Unnamed\" not in col and col != column_as_x:\n",
        "#             columns_strtegy.append(col)\n",
        "\n",
        "#     result.reindex()\n",
        "\n",
        "#     x = result[column_as_x].values.tolist()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (15, 6)\n",
        "#     # plt.figure()\n",
        "\n",
        "#     fig, ax = plt.subplots()\n",
        "#     colors = [\n",
        "#         \"black\",\n",
        "#         \"red\",\n",
        "#         \"green\",\n",
        "#         \"blue\",\n",
        "#         \"cyan\",\n",
        "#         \"magenta\",\n",
        "#         \"yellow\",\n",
        "#         \"aliceblue\",\n",
        "#         \"coral\",\n",
        "#         \"darksalmon\",\n",
        "#         \"firebrick\",\n",
        "#         \"honeydew\",\n",
        "#     ]\n",
        "#     for i in range(len(columns_strtegy)):\n",
        "#         col = columns_strtegy[i]\n",
        "#         ax.plot(\n",
        "#             x,\n",
        "#             result[col],\n",
        "#             color=colors[i],\n",
        "#             linewidth=1,\n",
        "#             linestyle=\"-\",\n",
        "#         )\n",
        "\n",
        "#     plt.title(\"\", fontsize=20)\n",
        "#     plt.xlabel(xlabel, fontsize=20)\n",
        "#     plt.ylabel(ylabel, fontsize=20)\n",
        "\n",
        "#     plt.legend(labels=columns_strtegy, loc=\"best\", fontsize=16)\n",
        "\n",
        "#     # set grid\n",
        "#     plt.grid()\n",
        "\n",
        "#     plt.xticks(size=22)  # 设置刻度大小\n",
        "#     plt.yticks(size=22)  # 设置刻度大小\n",
        "\n",
        "#     # #设置每隔多少距离⼀个刻度\n",
        "#     # plt.xticks(x[::60])\n",
        "\n",
        "#     # # 设置每月定位符\n",
        "#     # if if_set_x_monthlocator:\n",
        "#     #     ax.xaxis.set_major_locator(mdates.MonthLocator())  # interval = 1\n",
        "\n",
        "#     # 设置每隔多少距离⼀个刻度\n",
        "#     plt.xticks(x[::num_days_xticks])\n",
        "\n",
        "#     plt.setp(ax.get_xticklabels(), rotation=xrotation, horizontalalignment=\"center\")\n",
        "\n",
        "#     # 为防止x轴label重叠，自动调整label旋转角度\n",
        "#     if xrotation == 0:\n",
        "#         if_overlap = get_if_overlap(fig, ax)\n",
        "\n",
        "#         if if_overlap == True:\n",
        "#             plt.gcf().autofmt_xdate(ha=\"right\")  # ⾃动旋转⽇期标记\n",
        "\n",
        "#     plt.tight_layout()  # 自动调整子图间距\n",
        "\n",
        "#     plt.savefig(savefig_filename)\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# def get_if_overlap(fig, ax):\n",
        "#     fig.canvas.draw()\n",
        "#     # 获取日期标签的边界框\n",
        "#     bboxes = [label.get_window_extent() for label in ax.get_xticklabels()]\n",
        "#     # 计算日期标签之间的距离\n",
        "#     distances = [bboxes[i + 1].x0 - bboxes[i].x1 for i in range(len(bboxes) - 1)]\n",
        "#     # 如果有任何距离小于0，说明有重叠\n",
        "#     if any(distance < 0 for distance in distances):\n",
        "#         if_overlap = True\n",
        "#     else:\n",
        "#         if_overlap = False\n",
        "\n",
        "#     return if_overlap\n",
        "\n",
        "\n",
        "# def plot_return(\n",
        "#     result: pd.DataFrame(),\n",
        "#     column_as_x: str,\n",
        "#     if_need_calc_return: bool,\n",
        "#     savefig_filename: str = \"fig/result.png\",\n",
        "#     xlabel: str = \"Date\",\n",
        "#     ylabel: str = \"Return\",\n",
        "#     if_transfer_date: bool = True,\n",
        "#     select_start_date: str = None,\n",
        "#     select_end_date: str = None,\n",
        "#     num_days_xticks: int = 20,\n",
        "#     xrotation: int = 0,\n",
        "# ):\n",
        "#     if select_start_date is None:\n",
        "#         select_start_date: str = result[column_as_x].iloc[0]\n",
        "#         select_end_date: str = result[column_as_x].iloc[-1]\n",
        "#     # calc returns if if_need_calc_return is True, so that result stores returns\n",
        "#     select_start_date_index = result[column_as_x].tolist().index(select_start_date)\n",
        "#     columns = result.columns\n",
        "#     columns_strtegy = []\n",
        "#     column_as_x_index = None\n",
        "#     for i in range(len(columns)):\n",
        "#         col = columns[i]\n",
        "#         if col == column_as_x:\n",
        "#             column_as_x_index = i\n",
        "#         elif \"Unnamed\" not in col:\n",
        "#             columns_strtegy.append(col)\n",
        "#             if if_need_calc_return:\n",
        "#                 result[col] = result[col] / result[col][select_start_date_index] - 1\n",
        "\n",
        "#     # select the result between select_start_date and select_end_date\n",
        "#     # if date is 2020-01-15, transfer it to 01/15/2020\n",
        "#     num_rows, num_cols = result.shape\n",
        "#     tmp_result = copy.deepcopy(result)\n",
        "#     result = pd.DataFrame()\n",
        "#     if_first_row = True\n",
        "#     columns = []\n",
        "#     for i in range(num_rows):\n",
        "#         if (\n",
        "#             str2date(select_start_date)\n",
        "#             <= str2date(tmp_result[column_as_x][i])\n",
        "#             <= str2date(select_end_date)\n",
        "#         ):\n",
        "#             if \"-\" in tmp_result.iloc[i][column_as_x] and if_transfer_date:\n",
        "#                 new_date = transfer_date(tmp_result.iloc[i][column_as_x])\n",
        "#             else:\n",
        "#                 new_date = tmp_result.iloc[i][column_as_x]\n",
        "#             tmp_result.iloc[i, column_as_x_index] = new_date\n",
        "#             # print(\"tmp_result.iloc[i]: \", tmp_result.iloc[i])\n",
        "#             # result = result.append(tmp_result.iloc[i])\n",
        "#             if if_first_row:\n",
        "#                 columns = tmp_result.iloc[i].index.tolist()\n",
        "#                 result = pd.DataFrame(columns=columns)\n",
        "#                 # result = pd.concat([result, tmp_result.iloc[i]], axis=1)\n",
        "#                 # result = pd.DataFrame(tmp_result.iloc[i])\n",
        "#                 # result.columns = tmp_result.iloc[i].index.tolist()\n",
        "#                 if_first_row = False\n",
        "#             row = pd.DataFrame([tmp_result.iloc[i].tolist()], columns=columns)\n",
        "#             result = pd.concat([result, row], axis=0)\n",
        "\n",
        "#     # print final return of each strategy\n",
        "#     final_return = {}\n",
        "#     for col in columns_strtegy:\n",
        "#         final_return[col] = result.iloc[-1][col]\n",
        "#     print(\"final return: \", final_return)\n",
        "\n",
        "#     result.reindex()\n",
        "\n",
        "#     plot_result(\n",
        "#         result=result,\n",
        "#         column_as_x=column_as_x,\n",
        "#         savefig_filename=savefig_filename,\n",
        "#         xlabel=xlabel,\n",
        "#         ylabel=ylabel,\n",
        "#         num_days_xticks=num_days_xticks,\n",
        "#         xrotation=xrotation,\n",
        "#     )\n",
        "\n",
        "\n",
        "# def plot_return_from_csv(\n",
        "#     csv_file: str,\n",
        "#     column_as_x: str,\n",
        "#     if_need_calc_return: bool,\n",
        "#     savefig_filename: str = \"fig/result.png\",\n",
        "#     xlabel: str = \"Date\",\n",
        "#     ylabel: str = \"Return\",\n",
        "#     if_transfer_date: bool = True,\n",
        "#     select_start_date: str = None,\n",
        "#     select_end_date: str = None,\n",
        "#     num_days_xticks: int = 20,\n",
        "#     xrotation: int = 0,\n",
        "# ):\n",
        "#     result = pd.read_csv(csv_file)\n",
        "#     plot_return(\n",
        "#         result,\n",
        "#         column_as_x,\n",
        "#         if_need_calc_return,\n",
        "#         savefig_filename,\n",
        "#         xlabel,\n",
        "#         ylabel,\n",
        "#         if_transfer_date,\n",
        "#         select_start_date,\n",
        "#         select_end_date,\n",
        "#         num_days_xticks,\n",
        "#         xrotation,\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-58BUjVDwkf"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import datetime\n",
        "import os\n",
        "from datetime import date\n",
        "from datetime import timedelta\n",
        "from typing import List\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI4_ZKEeBFQa"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT1HyVP8AjtK"
      },
      "outputs": [],
      "source": [
        "# ## install finrl library\n",
        "# !pip install wrds\n",
        "# !pip install swig\n",
        "# !pip install 'shimmy>=2.0'\n",
        "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mnKtEacCSiZ"
      },
      "source": [
        "<a id='1.1'></a>\n",
        "## 2.1. Install all the packages through FinRL library\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR_IEVb9CYtB"
      },
      "source": [
        "\n",
        "<a id='1.2'></a>\n",
        "## 2.2. Check if the additional packages needed are present, if not install them.\n",
        "* Yahoo Finance API\n",
        "* pandas\n",
        "* numpy\n",
        "* matplotlib\n",
        "* stockstats\n",
        "* OpenAI gym\n",
        "* stable-baselines\n",
        "* tensorflow\n",
        "* pyfolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj0oMa7gCbdQ",
        "outputId": "52eddb16-25c7-4fda-83e0-3c4160430d20"
      },
      "outputs": [],
      "source": [
        "!pip install hyperopt\n",
        "from hyperopt import fmin, tpe, hp, Trials, space_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf0B-Rl6HiCr"
      },
      "outputs": [],
      "source": [
        "# #Importing the libraries\n",
        "# !pip install pandas_market_calendars\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib\n",
        "# import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')\n",
        "# import datetime\n",
        "# %matplotlib inline\n",
        "# from finrl import config\n",
        "# from finrl import config_tickers\n",
        "# from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "# from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "# from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "# from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv as StockTradingEnv_numpy\n",
        "# from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "# # from finrl.agents.rllib.models import DRLAgent as DRLAgent_rllib\n",
        "# from finrl.meta.data_processor import DataProcessor\n",
        "# import joblib\n",
        "# from stable_baselines3.common.logger import configure\n",
        "# from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline,convert_daily_return_to_pyfolio_ts\n",
        "# from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
        "# import ray\n",
        "# from pprint import pprint\n",
        "\n",
        "# import sys\n",
        "# sys.path.append(\"../FinRL-Library\")\n",
        "\n",
        "# import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJSOcS90Cpuk"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib\n",
        "# import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')\n",
        "# %matplotlib inline\n",
        "# import datetime\n",
        "\n",
        "# from finrl import config\n",
        "# from finrl import config_tickers\n",
        "# from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "# from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "# from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
        "# from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "# from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline,convert_daily_return_to_pyfolio_ts\n",
        "# from finrl.meta.data_processor import DataProcessor\n",
        "# from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
        "# import sys\n",
        "# sys.path.append(\"../FinRL-Library\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S35Ryj9ZCmKp"
      },
      "source": [
        "<a id='1.4'></a>\n",
        "## 2.4. Create Folders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3tcGYA8CuJ7"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
        "#     os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
        "# if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
        "#     os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
        "# if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
        "#     os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
        "# if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
        "#     os.makedirs(\"./\" + config.RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slBria_QbU4F"
      },
      "source": [
        "<a id='2'></a>\n",
        "# Part 3. Download Data\n",
        "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
        "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
        "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seSOy7GhC_2t",
        "outputId": "3be6992f-e01c-438f-b59e-07cd416753fa"
      },
      "outputs": [],
      "source": [
        "Nifty_ticker = ['RELIANCE.NS', 'ASIANPAINT.NS', 'BAJFINANCE.NS', 'HDFCBANK.NS', 'SBIN.NS']\n",
        "sensex_ticker = [\"ASIANPAINT.NS\", \"AXISBANK.NS\", \"BAJFINANCE.NS\", \"BAJAJFINSV.NS\", \"BHARTIARTL.NS\", \"HCLTECH.NS\", \"HDFCBANK.NS\",\n",
        "                 \"HINDUNILVR.NS\", \"ICICIBANK.NS\", \"INDUSINDBK.NS\", \"INFY.NS\", \"ITC.NS\", \"JSWSTEEL.NS\", \"KOTAKBANK.NS\", \"LT.NS\",\n",
        "                 \"M&M.NS\", \"MARUTI.NS\", \"NESTLEIND.NS\", \"NTPC.NS\", \"POWERGRID.NS\", \"RELIANCE.NS\", \"SBIN.NS\", \"SUNPHARMA.NS\",\n",
        "                 \"TATAMOTORS.NS\", \"TATASTEEL.NS\", \"TCS.NS\", \"TECHM.NS\", \"TITAN.NS\", \"ULTRACEMCO.NS\", \"WIPRO.NS\"]\n",
        "\n",
        "\n",
        "# BIST Turkey\n",
        "bist100_top30_tickers = ['AEFES.IS', 'AKBNK.IS', 'ARCLK.IS', 'ASELS.IS', 'BIMAS.IS', 'CCOLA.IS',\n",
        "       'DOHOL.IS', 'EKGYO.IS', 'ENKAI.IS', 'EREGL.IS', 'FROTO.IS', 'GARAN.IS',\n",
        "       'GOLTS.IS', 'HALKB.IS', 'ISCTR.IS', 'KCHOL.IS', 'KOZAL.IS', 'KRDMD.IS',\n",
        "       'PETKM.IS', 'SAHOL.IS', 'SISE.IS', 'TAVHL.IS', 'TCELL.IS', 'THYAO.IS',\n",
        "       'TKFEN.IS', 'TOASO.IS', 'TTKOM.IS', 'TUPRS.IS', 'ULKER.IS', 'VAKBN.IS',\n",
        "       'VESTL.IS', 'YKBNK.IS']\n",
        "\n",
        "# Spain IBEX top 30\n",
        "ibex35_tickers = ['ACS.MC', 'ACX.MC', 'AMS.MC', 'ANA.MC', 'BBVA.MC', 'BKT.MC', 'CABK.MC',\n",
        "       'COL.MC', 'ELE.MC', 'ENG.MC', 'FDR.MC', 'FER.MC', 'GRF.MC', 'IBE.MC',\n",
        "       'IDR.MC', 'ITX.MC', 'MAP.MC', 'MEL.MC', 'MTS.MC', 'NTGY.MC', 'RED.MC',\n",
        "       'REP.MC', 'ROVI.MC', 'SAB.MC', 'SAN.MC', 'SCYR.MC', 'SLR.MC', 'TEF.MC']\n",
        "\n",
        "# Tickers for the top 30 stocks on B3 (Brasil Bolsa Balcão)\n",
        "\n",
        "brazil_tickers = ['ABEV3.SA', 'BBAS3.SA', 'BPAN4.SA', 'BRFS3.SA', 'BRKM5.SA', 'CSNA3.SA',\n",
        "       'CYRE3.SA', 'ECOR3.SA', 'EGIE3.SA', 'ELET3.SA', 'ELET6.SA', 'EMBR3.SA',\n",
        "       'EQTL3.SA', 'GGBR4.SA', 'ITUB4.SA', 'JBSS3.SA', 'LREN3.SA',\n",
        "       'MRFG3.SA', 'PETR3.SA', 'PETR4.SA', 'RADL3.SA', 'RENT3.SA', 'SBSP3.SA',\n",
        "       'SUZB3.SA', 'UGPA3.SA', 'USIM5.SA', 'VALE3.SA', 'WEGE3.SA', 'YDUQ3.SA']\n",
        "\n",
        "\n",
        "# Final Tickers Hang Seng (Hong Kong)\n",
        "hang_seng_symbols = ['0002.HK', '0003.HK', '0012.HK', '0017.HK', '0027.HK', '0101.HK',\n",
        "       '0241.HK', '0267.HK', '0669.HK', '0762.HK', '0836.HK', '0883.HK',\n",
        "       '0906.HK', '0939.HK', '0992.HK', '1038.HK', '1044.HK', '1093.HK',\n",
        "       '1109.HK', '1398.HK', '2020.HK', '2319.HK', '2331.HK', '2382.HK',\n",
        "       '2628.HK', '2688.HK', '3323.HK', '3328.HK', '3983.HK', '3988.HK']\n",
        "\n",
        "# Tiwan TWSE Market\n",
        "twse_top30 = ['1216.TW', '1301.TW', '1303.TW', '1519.TW', '1537.TW', '2308.TW',\n",
        "       '2317.TW', '2330.TW', '2363.TW', '2368.TW', '2382.TW', '2412.TW',\n",
        "       '2454.TW', '2474.TW', '2504.TW', '2603.TW', '2838.TW', '2880.TW',\n",
        "       '2881.TW', '2882.TW', '2884.TW', '2886.TW', '2891.TW', '2892.TW',\n",
        "       '3008.TW', '3045.TW', '3653.TW', '4904.TW', '5880.TW', '6505.TW']\n",
        "# UK FTSE top 30 working Stock\n",
        "FTSE_top30 = ['ABF.L', 'ADM.L', 'AHT.L', 'AV.L', 'BA.L', 'BEZ.L', 'CCL.L', 'CNA.L',\n",
        "       'DPLM.L', 'ENT.L', 'FRAS.L', 'HSBA.L', 'HWDN.L', 'III.L',\n",
        "       'IMI.L', 'INF.L', 'MKS.L', 'MRO.L', 'NXT.L', 'PSON.L', 'REL.L', 'RR.L',\n",
        "       'SBRY.L', 'SKG.L', 'SMDS.L', 'SMIN.L', 'SMT.L', 'SPX.L', 'SSE.L']\n",
        "# Japanies Nikkei Top 30\n",
        "nikkei_top30_symbols = ['2914.T', '3382.T', '3407.T', '3861.T', '4063.T', '4502.T', '4689.T',\n",
        "       '4755.T', '5802.T', '6301.T', '6471.T', '6501.T', '6594.T', '6701.T',\n",
        "       '6758.T', '6920.T', '7011.T', '7203.T', '7267.T', '7735.T', '7974.T',\n",
        "       '8031.T', '8035.T', '8058.T', '8306.T', '8316.T', '9020.T', '9022.T',\n",
        "       '9983.T', '9984.T']\n",
        "# German DAX top 30\n",
        "dax_30 = ['ADS.DE', 'AIR.DE', 'ALV.DE', 'BAS.DE', 'BEI.DE', 'BMW.DE', 'BNR.DE',\n",
        "       'BOSS.DE', 'CBK.DE', 'CON.DE', 'DB1.DE', 'DBK.DE', 'DTE.DE', 'DWNI.DE',\n",
        "       'EOAN.DE', 'EVT.DE', 'FME.DE', 'FNTN.DE', 'FRE.DE', 'HEI.DE', 'HNR1.DE',\n",
        "       'LIN.DE', 'MRK.DE', 'MTX.DE', 'MUV2.DE', 'SAP.DE', 'SIE.DE', 'SY1.DE',\n",
        "       'TL0.DE', 'VOW3.DE']\n",
        "# USA Dow 30\n",
        "Dow_30 = ['AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS', 'GS',\n",
        "       'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM', 'MRK',\n",
        "       'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT']\n",
        "\n",
        "indices= [sensex_ticker, Dow_30, dax_30, nikkei_top30_symbols, FTSE_top30, twse_top30, hang_seng_symbols, brazil_tickers, ibex35_tickers, bist100_top30_tickers ]\n",
        "\n",
        "\n",
        "# Download and save the data in a pandas DataFrame:\n",
        "df = YahooDownloader(start_date = '2011-01-01',\n",
        "                     end_date = '2025-02-28',\n",
        "                     ticker_list = sensex_ticker).fetch_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoTg1NzMDK9Z",
        "outputId": "b39d6428-02dc-4179-f777-3dcb76bae9f7"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6sWPoN3DNhA"
      },
      "outputs": [],
      "source": [
        "from stockstats import StockDataFrame as Sdf\n",
        "\n",
        "def add_tech(data, INDICATORS):\n",
        "  df = data.copy()\n",
        "  df = df.sort_values(by=[\"tic\", \"date\"])\n",
        "  stock = Sdf.retype(df.copy())\n",
        "  unique_ticker = stock.tic.unique()\n",
        "\n",
        "  for indicator in INDICATORS:\n",
        "      indicator_df = pd.DataFrame()\n",
        "      for i in range(len(unique_ticker)):\n",
        "          try:\n",
        "              temp_indicator = stock[stock.tic == unique_ticker[i]][indicator]\n",
        "              temp_indicator = pd.DataFrame(temp_indicator)\n",
        "              temp_indicator[\"tic\"] = unique_ticker[i]\n",
        "              temp_indicator[\"date\"] = df[df.tic == unique_ticker[i]][\n",
        "                  \"date\"\n",
        "              ].to_list()\n",
        "              # indicator_df = indicator_df.append(\n",
        "              #     temp_indicator, ignore_index=True\n",
        "              # )\n",
        "              indicator_df = pd.concat(\n",
        "                  [indicator_df, temp_indicator], axis=0, ignore_index=True\n",
        "              )\n",
        "          except Exception as e:\n",
        "              print(e)\n",
        "      df = df.merge(\n",
        "          indicator_df[[\"tic\", \"date\", indicator]], on=[\"tic\", \"date\"], how=\"left\"\n",
        "      )\n",
        "\n",
        "  df = df.sort_values(by=[\"date\", \"tic\"])\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU6Iyi0rDWuO"
      },
      "outputs": [],
      "source": [
        "INDICATORS = ['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30', 'close_30_sma', 'close_60_sma']\n",
        "df = add_tech(df, INDICATORS)\n",
        "df = df.ffill().bfill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bmnIGHkDa1b"
      },
      "outputs": [],
      "source": [
        "# add covariance matrix as states\n",
        "df=df.sort_values(['date','tic'],ignore_index=True)\n",
        "df.index = df.date.factorize()[0]\n",
        "\n",
        "cov_list = []\n",
        "return_list = []\n",
        "\n",
        "# look back is one year\n",
        "lookback=252\n",
        "for i in range(lookback,len(df.index.unique())):\n",
        "  data_lookback = df.loc[i-lookback:i,:]\n",
        "  price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n",
        "  return_lookback = price_lookback.pct_change().dropna()\n",
        "  return_list.append(return_lookback)\n",
        "\n",
        "  covs = return_lookback.cov().values\n",
        "  cov_list.append(covs)\n",
        "\n",
        "\n",
        "df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
        "df = df.merge(df_cov, on='date')\n",
        "df = df.sort_values(['date','tic']).reset_index(drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "8fTc_HkhwP0w",
        "outputId": "6cccebe0-8d45-4936-f99a-eb25785ce060"
      },
      "outputs": [],
      "source": [
        "df['cov_list'].head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-Uj3umRuDdKE",
        "outputId": "adbd7301-c1b0-491e-f3ce-0bcff662d030"
      },
      "outputs": [],
      "source": [
        "df.head(50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "GiegXIXckmyo",
        "outputId": "9773e1ee-8bc1-47e4-e51a-9bda527002dc"
      },
      "outputs": [],
      "source": [
        "df['return_list'].values[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn_ERBJwDdzc",
        "outputId": "a51d1ed9-0564-4b36-9b40-95f611fc99f4"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "\n",
        "hist_vol=[]\n",
        "for i in range(len(df['return_list'])):\n",
        "  returns = df['return_list'].values[i].std()\n",
        "  hist_vol.append(returns)\n",
        "print(len(hist_vol))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF-yUUealo0k"
      },
      "outputs": [],
      "source": [
        "hist_vol= np.array(hist_vol)\n",
        "# print(hist_vol.shape)\n",
        "# print(hist_vol)\n",
        "hist_vol= pd.DataFrame(hist_vol, df['date'])\n",
        "# print(hist_vol.shape)\n",
        "# print(df)\n",
        "# df.to_csv('sensex_data.csv')\n",
        "# hist_vol.to_csv('sensex_hist_vol.csv')\n",
        "# from google.colab import files\n",
        "# files.download('sensex_data.csv')\n",
        "# files.download('sensex_hist_vol.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRUbLRw5obqh"
      },
      "outputs": [],
      "source": [
        "# df= pd.read_csv(\"sensex_data.csv\")\n",
        "# hist_vol= pd.read_csv(\"sensex_hist_vol.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1gN78rWD3JV"
      },
      "source": [
        "<a id='4'></a>  \n",
        "# **Part 5. Design Environment**  \n",
        "\n",
        "We model portfolio optimization as a **Constrained Markov Decision Process (CMDP)**, ensuring **maximum returns while controlling risk**. The **state** includes stock prices, indicators, and risk metrics. The **action** is portfolio allocation, adjusting stock weights within constraints. **Rewards** maximize returns, while a **cost function** penalizes excessive risk. **Deep Deterministic Policy Gradient (DDPG) with Augmented Lagrangian Multiplier (ALM)** trains the agent:  \n",
        "✅ **Actor** optimizes allocations.  \n",
        "✅ **Critic** evaluates returns.  \n",
        "✅ **Cost network** estimates risk.  \n",
        "✅ **Lagrangian multiplier** enforces constraints.  \n",
        "This ensures **risk-aware, reinforcement learning-based trading**, scalable to real markets.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlmFmlKVD35D",
        "outputId": "9ff2fd91-7c62-4812-f80a-53b07417db4f"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip install shimmy\n",
        "!pip install stable_baselines3\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwqCCWiQD4h1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gym.utils import seeding\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQnmN1qdk88I"
      },
      "source": [
        "## Training data split: 2009-01-01 to 2020-07-01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38oomnmzEH57",
        "outputId": "0dcf3033-ed0f-4df4-dbb1-260b0c072087"
      },
      "outputs": [],
      "source": [
        "TRAIN_START_DATE = '2011-01-01'\n",
        "TRAIN_END_DATE = '2021-12-31'\n",
        "\n",
        "# TRAIN_END_DATE = '2012-12-01'\n",
        "\n",
        "Val_START_DATE = '2022-01-01'\n",
        "VAL_END_DATE =  '2022-12-31'\n",
        "TRADE_START_DATE = '2023-01-01'\n",
        "TRADE_END_DATE = '2025-02-28'\n",
        "# print(df[30:])\n",
        "# hist_vol = hist_vol.reset_index(drop=True)\n",
        "\n",
        "train = data_split(df, TRAIN_START_DATE,TRAIN_END_DATE)\n",
        "hist_vol_train = hist_vol[TRAIN_START_DATE : TRAIN_END_DATE]\n",
        "\n",
        "val = data_split(df, Val_START_DATE, VAL_END_DATE)\n",
        "hist_vol_val=hist_vol[Val_START_DATE :VAL_END_DATE]\n",
        "\n",
        "full_train = data_split(df, TRAIN_START_DATE, VAL_END_DATE)\n",
        "hist_vol_full_train= hist_vol[TRAIN_START_DATE :VAL_END_DATE]\n",
        "\n",
        "\n",
        "# full_train = data_split(df, TRAIN_START_DATE,TRAIN_END_DATE)\n",
        "# hist_vol_full_train= hist_vol[TRAIN_START_DATE :TRAIN_END_DATE]\n",
        "\n",
        "trade = data_split(df, TRADE_START_DATE,TRADE_END_DATE)\n",
        "hist_vol_trade= hist_vol[TRADE_START_DATE  : TRADE_END_DATE]\n",
        "\n",
        "print(full_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RYE3YvyfVMb"
      },
      "source": [
        "Here is the definition of the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRYTE_PNEO06"
      },
      "outputs": [],
      "source": [
        "class StockPortfolioEnv(gym.Env):\n",
        "    \"\"\"A single stock trading environment for OpenAI gym\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        df: DataFrame\n",
        "            input data\n",
        "        stock_dim : int\n",
        "            number of unique stocks\n",
        "        hmax : int\n",
        "            maximum number of shares to trade\n",
        "        initial_amount : int\n",
        "            start money\n",
        "        transaction_cost_pct: float\n",
        "            transaction cost percentage per trade\n",
        "        reward_scaling: float\n",
        "            scaling factor for reward, good for training\n",
        "        state_space: int\n",
        "            the dimension of input features\n",
        "        action_space: int\n",
        "            equals stock dimension\n",
        "        tech_indicator_list: list\n",
        "            a list of technical indicator names\n",
        "        turbulence_threshold: int\n",
        "            a threshold to control risk aversion\n",
        "        day: int\n",
        "            an increment number to control date\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    _sell_stock()\n",
        "        perform sell action based on the sign of the action\n",
        "    _buy_stock()\n",
        "        perform buy action based on the sign of the action\n",
        "    step()\n",
        "        at each step the agent will return actions, then\n",
        "        we will calculate the reward, and return the next observation.\n",
        "    reset()\n",
        "        reset the environment\n",
        "    render()\n",
        "        use render to return other functions\n",
        "    save_asset_memory()\n",
        "        return account value at each time step\n",
        "    save_action_memory()\n",
        "        return actions/positions at each time step\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self,\n",
        "                df,\n",
        "                stock_dim,\n",
        "                hmax,\n",
        "                initial_amount,\n",
        "                transaction_cost_pct,\n",
        "                reward_scaling,\n",
        "                state_space,\n",
        "                action_space,\n",
        "                tech_indicator_list,\n",
        "                turbulence_threshold=None,\n",
        "                lookback=252,\n",
        "                day = 0, hist_vol= None):\n",
        "\n",
        "        self.day = day\n",
        "        self.lookback=lookback\n",
        "        self.df = df\n",
        "        self.stock_dim = stock_dim\n",
        "        self.hmax = hmax\n",
        "        self.initial_amount = initial_amount\n",
        "        self.transaction_cost_pct =transaction_cost_pct\n",
        "        self.reward_scaling = reward_scaling\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.tech_indicator_list = tech_indicator_list\n",
        "        self.hist_vol=hist_vol\n",
        "        self.DSR_A = 0.0\n",
        "        self.DSR_B = 0.0\n",
        "\n",
        "         # action_space normalization and shape is self.stock_dim\n",
        "        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,))\n",
        "\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+1 + len(self.tech_indicator_list), self.state_space))\n",
        "\n",
        "\n",
        "\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.covs = self.data['cov_list'].values[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.state = np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        # print(\" state  :: \" , self.day ,self.state.shape, self.state)\n",
        "        # print(\" hist_ vol  :: \" , self.day , type(self.hist_vol), self.hist_vol)\n",
        "\n",
        "        hist_volll = self.hist_vol.values[self.day,:]\n",
        "        # Concatenate along axis=0\n",
        "\n",
        "        self.state = np.concatenate([self.state, hist_volll.reshape(1,-1) ], axis=0)\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"states - \" , self.state.shape)\n",
        "\n",
        "        self.terminal = False\n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
        "        self.portfolio_value = self.initial_amount\n",
        "\n",
        "        # memorize portfolio value each step\n",
        "        self.asset_memory = [self.initial_amount]\n",
        "        # memorize portfolio return each step\n",
        "        self.portfolio_return_memory = [0]\n",
        "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
        "        self.date_memory=[self.data.date.unique()[0]]\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, actions):\n",
        "      print(f\" the len of the df is  {len(self.df.index.unique())}  and the current day is :  {self.day } and  if  terminal is  : { self.day >= len(self.df.index.unique()) - 1 }\")\n",
        "      self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
        "\n",
        "      if self.terminal:\n",
        "          # print(\"=================================\")\n",
        "          # print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
        "          # print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
        "          # return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "\n",
        "          df = pd.DataFrame(self.portfolio_return_memory)\n",
        "          df.columns = ['daily_return']\n",
        "          # plt.plot(df.daily_return.cumsum(),'r')\n",
        "          # plt.savefig('results/cumulative_reward.png')\n",
        "          # plt.close()\n",
        "\n",
        "          # plt.plot(self.portfolio_return_memory,'r')\n",
        "          # plt.savefig('results/rewards.png')\n",
        "          # plt.close()\n",
        "\n",
        "          print(\"=================================\")\n",
        "          print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
        "          print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
        "\n",
        "          df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
        "          df_daily_return.columns = ['daily_return']\n",
        "          if df_daily_return['daily_return'].std() !=0:\n",
        "            sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n",
        "                    df_daily_return['daily_return'].std()\n",
        "            print(\"Sharpe: \",sharpe)\n",
        "          print(\"=================================\")\n",
        "\n",
        "\n",
        "          return self.state, self.reward, self.terminal,{}\n",
        "      else:\n",
        "          last_day_memory = self.data\n",
        "          weights = self.softmax_normalization(actions)  # Ensure valid portfolio weights\n",
        "          self.actions_memory.append(weights)\n",
        "\n",
        "          # Load next state\n",
        "          self.day = self.day+ 1\n",
        "          self.data = self.df.loc[self.day, :]\n",
        "          self.covs = self.data['cov_list'].values[0]\n",
        "          self.state = np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "          hist_voll= self.hist_vol.values[self.day,:]\n",
        "          self.state = np.concatenate([self.state, hist_voll.reshape(1,-1) ], axis=0)\n",
        "\n",
        "          # Portfolio Value Update\n",
        "          portfolio_return = sum(((self.data.close.values / last_day_memory.close.values) - 1) * weights)\n",
        "          new_portfolio_value = self.portfolio_value * (1 + portfolio_return)\n",
        "\n",
        "          # Calculate Transaction Fee\n",
        "          phi = 0.0025  # 0.25% transaction cost\n",
        "          # Reshape portfolio_value to match dimensions of other arrays\n",
        "          portfolio_value_reshaped = np.repeat(self.portfolio_value, len(weights))\n",
        "          transaction_fee = phi * sum(\n",
        "              abs(weights * new_portfolio_value * last_day_memory.close.values / self.data.close.values\n",
        "                  - self.actions_memory[-2] * portfolio_value_reshaped)  # Use portfolio_value_reshaped\n",
        "          )\n",
        "\n",
        "          # Reward Calculation\n",
        "          self.reward = (new_portfolio_value - self.portfolio_value) - transaction_fee  # r_t = u_t - u_t-1 - fee_t\n",
        "\n",
        "          # Update portfolio value\n",
        "          self.portfolio_value = new_portfolio_value\n",
        "\n",
        "          # Save to memory\n",
        "          self.portfolio_return_memory.append(portfolio_return)\n",
        "          self.asset_memory.append(new_portfolio_value)\n",
        "          self.date_memory.append(self.data.date.unique()[0])\n",
        "\n",
        "          return self.state, self.reward, self.terminal, {}\n",
        "    ##############################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.asset_memory = [self.initial_amount]\n",
        "        self.day = 0\n",
        "\n",
        "        # returns = self.df['return_list'].values[0]\n",
        "        # hist_vol = returns.rolling(window=30).std()\n",
        "        # hist_vol.fillna(0, inplace=True)\n",
        "        # hist_vol = hist_vol.iloc[self.day,:]\n",
        "\n",
        "\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        # load states\n",
        "        self.covs = self.data['cov_list'].values[0]\n",
        "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        # print(self.hist_vol)\n",
        "        # self.hist_vol= self.hist_vol[self.day,]\n",
        "        # Concatenate along axis=0\n",
        "\n",
        "        hist_voll= self.hist_vol.values[self.day,:]\n",
        "        self.state = np.concatenate([self.state, hist_voll.reshape(1,-1)], axis=0)\n",
        "        # Concatenate along axis=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # print(\" reset -- ev  --state -\", self.state.shape)\n",
        "        # print(\" reset -- ev -- state - \", self.state)\n",
        "        # print(\" reset -- ev-- cov - \", self.state[:30, :].shape)\n",
        "        # print(\" reset -- ev-- his vol- \", self.state[:-1, :].shape)\n",
        "        # print(\" reset -- ev-- his vol- \", self.state[-1:, :])\n",
        "        self.portfolio_value = self.initial_amount\n",
        "        #self.cost = 0\n",
        "        #self.trades = 0\n",
        "        self.DSR_A = 0.0\n",
        "        self.DSR_B = 0.0\n",
        "        self.terminal = False\n",
        "        self.portfolio_return_memory = [0]\n",
        "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
        "        self.date_memory=[self.data.date.unique()[0]]\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        return self.state\n",
        "\n",
        "    def softmax_normalization(self, actions):\n",
        "        numerator = np.exp(actions)\n",
        "        denominator = np.sum(np.exp(actions))\n",
        "        softmax_output = numerator/denominator\n",
        "        return softmax_output\n",
        "\n",
        "\n",
        "    def apply_dirichlet_noise(self, actions, alpha=0.1):\n",
        "      \"\"\"\n",
        "      Apply Dirichlet noise to actions to encourage exploration.\n",
        "\n",
        "      Args:\n",
        "      - actions (np.array): Original action values from the RL model.\n",
        "      - alpha (float): Dirichlet concentration parameter. Lower values = more noise.\n",
        "\n",
        "      Returns:\n",
        "      - np.array: Modified action values with noise, ensuring sum = 1.\n",
        "      \"\"\"\n",
        "      noise = np.random.dirichlet([alpha] * len(actions))  # Sample from Dirichlet distribution\n",
        "      noisy_actions = 0.75 * actions + 0.25 * noise  # Blend original actions with noise\n",
        "      return noisy_actions / noisy_actions.sum()  # Normalize to ensure sum = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_asset_memory(self):\n",
        "        date_list = self.date_memory\n",
        "        portfolio_return = self.portfolio_return_memory\n",
        "        #print(len(date_list))\n",
        "        #print(len(asset_list))\n",
        "        df_account_value = pd.DataFrame({'date':date_list,'daily_return':portfolio_return})\n",
        "        return df_account_value\n",
        "\n",
        "    def save_action_memory(self):\n",
        "        # date and close price length must match actions length\n",
        "        date_list = self.date_memory\n",
        "        df_date = pd.DataFrame(date_list)\n",
        "        df_date.columns = ['date']\n",
        "\n",
        "        action_list = self.actions_memory\n",
        "        df_actions = pd.DataFrame(action_list)\n",
        "        df_actions.columns = self.data.tic.values\n",
        "        df_actions.index = df_date.date\n",
        "        #df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
        "        return df_actions\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def get_sb_env(self):\n",
        "        e = DummyVecEnv([lambda: self])\n",
        "        obs = e.reset()\n",
        "        return e, obs\n",
        "\n",
        "    def calculate_DSR(self, R):\n",
        "      eta = 0.004\n",
        "      delta_A = R - self.DSR_A\n",
        "      delta_B = R**2 - self.DSR_B\n",
        "      Dt = (self.DSR_B*delta_A - 0.5*self.DSR_A*delta_B) / ((self.DSR_B-self.DSR_A**2)**(3/2) + 1e-6)\n",
        "      self.DSR_A = self.DSR_A + eta*delta_A\n",
        "      self.DSR_B = self.DSR_B + eta*delta_B\n",
        "      return(Dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rzwClQuKPkN",
        "outputId": "2ad2f73e-1761-4218-daa5-6792e9df5255"
      },
      "outputs": [],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "\n",
        "state_space = stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuJCpxmYKS9x"
      },
      "outputs": [],
      "source": [
        "# print(INDICATORS)\n",
        "TURBULENCE_THRESHOLD= 0.0020\n",
        "\n",
        "env_kwargs_train = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"transaction_cost_pct\": 0.001,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"hist_vol\":hist_vol_train,\n",
        "    'turbulence_threshold': TURBULENCE_THRESHOLD\n",
        "\n",
        "}\n",
        "# print(hist_vol_val,\"  ddddd \")\n",
        "env_kwargs_val = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"transaction_cost_pct\": 0.001,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"hist_vol\":hist_vol_val,\n",
        "    \"turbulence_threshold\": TURBULENCE_THRESHOLD\n",
        "}\n",
        "\n",
        "env_kwargs_full = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"transaction_cost_pct\": 0.001,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"hist_vol\":hist_vol_full_train,\n",
        "    \"turbulence_threshold\": TURBULENCE_THRESHOLD\n",
        "}\n",
        "\n",
        "env_kwargs_trade = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"transaction_cost_pct\": 0.001,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"hist_vol\":hist_vol_trade,\n",
        "    \"turbulence_threshold\": TURBULENCE_THRESHOLD\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IBWBb6kKVTo",
        "outputId": "8a5bbbfd-36f1-46e3-fc5b-e95ee6283fcd"
      },
      "outputs": [],
      "source": [
        "e_train_gym = StockPortfolioEnv(df = train, **env_kwargs_train)\n",
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "\n",
        "e_val_gym = StockPortfolioEnv(df = val, **env_kwargs_val)\n",
        "env_val, _ = e_val_gym.get_sb_env()\n",
        "\n",
        "e_train_full_gym = StockPortfolioEnv(df = full_train, **env_kwargs_full)\n",
        "env_full_train, _ = e_train_full_gym.get_sb_env()\n",
        "\n",
        "e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs_trade)\n",
        "env_trade, _ = e_trade_gym.get_sb_env()\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eKIu5UPlPnk"
      },
      "source": [
        "<a id='5'></a>\n",
        "# Part 6: Implement DRL Algorithms\n",
        "* DDPG-with ALM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkVzI8KqK7u4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        experience = (state, action, np.array([reward]), next_state, done)\n",
        "\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state_batch = []\n",
        "        action_batch = []\n",
        "        reward_batch = []\n",
        "        next_state_batch = []\n",
        "        done_batch = []\n",
        "\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        for experience in batch:\n",
        "            state, action, reward, next_state, done = experience\n",
        "            state_batch.append(state)\n",
        "            action_batch.append(action)\n",
        "            reward_batch.append(reward)\n",
        "            next_state_batch.append(next_state)\n",
        "            done_batch.append(done)\n",
        "\n",
        "        state_batch = np.array(state_batch)\n",
        "        action_batch = np.array(action_batch)\n",
        "        reward_batch = np.array(reward_batch)\n",
        "        next_state_batch = np.array(next_state_batch)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySftV0MuLVKP"
      },
      "source": [
        "* using dirchilet noise instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEQnh6M5LBKi"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Noise(action, action_space, kappa=10):\n",
        "    \"\"\"\n",
        "    Apply Dirichlet noise for exploration in DDPG according to the paper.\n",
        "\n",
        "    Args:\n",
        "    - action (torch.Tensor): Original action values from the actor network.\n",
        "    - action_space (gym.spaces.Box): Action space defining valid ranges.\n",
        "    - kappa (float): Controls exploration variance. Higher kappa = less noise.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: Modified action values with Dirichlet noise, ensuring sum = 1.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Ensure actions are non-negative before applying Dirichlet noise\n",
        "        action = torch.clamp(action, min=0.0)\n",
        "\n",
        "        # Convert actions to numpy array for Dirichlet sampling\n",
        "        action_np = action.detach().cpu().numpy()\n",
        "\n",
        "        # Compute shape parameter: υ = κ * a\n",
        "        upsilon = kappa * action_np\n",
        "\n",
        "        # Ensure upsilon is positive and correctly shaped\n",
        "        upsilon = np.maximum(upsilon, 1e-6)  # Prevent zero or negative values\n",
        "        upsilon = upsilon.flatten()  # Ensure it's a 1D array\n",
        "\n",
        "        # Debugging: Check upsilon values\n",
        "        if np.any(upsilon <= 0):\n",
        "            raise ValueError(f\"Dirichlet parameters must be positive. Found: {upsilon}\")\n",
        "\n",
        "        # Sample ϵ from Dirichlet distribution\n",
        "        epsilon = np.random.dirichlet(upsilon)\n",
        "\n",
        "        # Compute final action: a' = a + sg(ϵ - a)\n",
        "        noisy_action = action_np + (epsilon - action_np)\n",
        "\n",
        "        # Apply StopGradient (detach the noise term)\n",
        "        noisy_action = action_np + torch.tensor(noisy_action - action_np, requires_grad=False).numpy()\n",
        "\n",
        "        # Clip extreme values to prevent instability\n",
        "        noisy_action = np.clip(noisy_action, 0.0, 1.0)\n",
        "\n",
        "        # Ensure sum = 1 for valid portfolio allocation\n",
        "        noisy_action = noisy_action / noisy_action.sum()\n",
        "\n",
        "        return noisy_action\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"ValueError in Dirichlet noise function: {ve}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in Dirichlet noise function: {e}\")\n",
        "\n",
        "    # Return the original action if an error occurs\n",
        "    return action.detach().cpu().numpy()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLw3cw3fO4SF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, num_layers, act_fn, dr):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        if act_fn == 'relu': activation_fn = nn.ReLU()\n",
        "        if act_fn == 'tanh': activation_fn = nn.Tanh()\n",
        "        if act_fn == 'sigmoid': activation_fn = nn.Sigmoid()\n",
        "        # print(\"Params Dictionary:\", self.params)\n",
        "        hidden_dim = int(hidden_dim)\n",
        "        num_layers = int(num_layers)\n",
        "        action_dim = int(action_dim)\n",
        "        state_dim = int(state_dim)\n",
        "\n",
        "        # print(\"state_dim:\", state_dim)\n",
        "        # print(\"action_dim:\", action_dim)\n",
        "        # print(\"hidden_dim:\", hidden_dim)\n",
        "        # print(\"num_layers:\", num_layers)\n",
        "        # print(\"act_fn:\", act_fn)\n",
        "        # print(\"dr:\", dr)\n",
        "        # print(f\"state_dim: {state_dim}, type: {type(state_dim)}\")\n",
        "        # print(f\"action_dim: {action_dim}, type: {type(action_dim)}\")\n",
        "        # print(f\"hidden_dim: {hidden_dim}, type: {type(hidden_dim)}\")\n",
        "\n",
        "        # Add input layer\n",
        "\n",
        "        layers.append(nn.Flatten())\n",
        "        layers.append(nn.Linear(state_dim, hidden_dim))\n",
        "        layers.append(activation_fn)\n",
        "        layers.append(nn.Dropout(p=dr))\n",
        "\n",
        "        # Add hidden layers\n",
        "        for _ in range(num_layers - 2):  # -2 because we already added the input and output layers\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(activation_fn)\n",
        "            layers.append(nn.Dropout(p=dr))\n",
        "\n",
        "        # Add output layer\n",
        "        layers.append(nn.Linear(hidden_dim, action_dim))\n",
        "        # layers.append(nn.Dropout(p=dr))\n",
        "\n",
        "        # Create the sequential model\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        x = self.model(state)\n",
        "        x = torch.tanh(x)\n",
        "        # print(\" actor  Network forward (((((((((((((((((((((((((((((((((((((())))))))))))))))))))))))))))))))))))))\")\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, num_layers, act_fn, dr):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        if act_fn == 'relu': activation_fn = nn.ReLU()\n",
        "        if act_fn == 'tanh': activation_fn = nn.Tanh()\n",
        "        if act_fn == 'sigmoid': activation_fn = nn.Sigmoid()\n",
        "        hidden_dim = int(hidden_dim)\n",
        "        num_layers = int(num_layers)\n",
        "        action_dim = int(action_dim)\n",
        "        state_dim = int(state_dim)\n",
        "\n",
        "        # print(\"state_dim:\", state_dim)\n",
        "        # print(\"action_dim:\", action_dim)\n",
        "        # print(\"hidden_dim:\", hidden_dim)\n",
        "        # print(\"num_layers:\", num_layers)\n",
        "        # print(\"act_fn:\", act_fn)\n",
        "        # print(\"dr:\", dr)\n",
        "        # print(f\"state_dim: {state_dim}, type: {type(state_dim)}\")\n",
        "        # print(f\"action_dim: {action_dim}, type: {type(action_dim)}\")\n",
        "        # print(f\"hidden_dim: {hidden_dim}, type: {type(hidden_dim)}\")\n",
        "\n",
        "\n",
        "        # Add input layer\n",
        "        layers.append(nn.Linear(state_dim + action_dim, hidden_dim))\n",
        "        layers.append(activation_fn)\n",
        "        layers.append(nn.Dropout(p=dr))\n",
        "\n",
        "        # Add hidden layers\n",
        "        for _ in range(num_layers - 2):  # -2 because we already added the input and output layers\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(activation_fn)\n",
        "            layers.append(nn.Dropout(p=dr))\n",
        "\n",
        "        # Add output layer\n",
        "        # layers.append(nn.Dropout(p=dr))\n",
        "        layers.append(nn.Linear(hidden_dim, 1))\n",
        "\n",
        "        # Create the sequential model\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"\n",
        "        Forward pass of the Critic network.\n",
        "\n",
        "        Args:\n",
        "        - state (torch.Tensor): State tensor.\n",
        "        - action (torch.Tensor): Action tensor.\n",
        "\n",
        "        Returns:\n",
        "        - Q-value estimation.\n",
        "        \"\"\"\n",
        "\n",
        "        # 🔍 Print debug info\n",
        "        # print(\"Critic Network forward (((((((((((((((((((((((((((((((((((((())))))))))))))))))))))))))))))))))))))\")\n",
        "        # print(f\"State shape before reshape: {state.shape}, Action shape before reshape: {action.shape}\")\n",
        "\n",
        "        # 🔄 Flatten state if it has more than 2 dimensions (CNN case)\n",
        "        if state.dim() > 2:\n",
        "            state = state.view(state.shape[0], -1)  # Convert to (batch_size, features)\n",
        "\n",
        "        # 🔄 Ensure action is 2D\n",
        "        if action.dim() > 2:\n",
        "            action = action.view(action.shape[0], -1)  # Convert to (batch_size, action_dim)\n",
        "\n",
        "        # 🔍 Print final shapes\n",
        "        # print(f\"State shape after reshape: {state.shape}, Action shape after reshape: {action.shape}\")\n",
        "\n",
        "        # ✅ Now both state and action are 2D → Safe to concatenate\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "\n",
        "        # Forward pass through Critic layers\n",
        "        x = self.model(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CostNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for estimating portfolio risk (cost).\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super(CostNetwork, self).__init__()\n",
        "\n",
        "        state_dim=int(state_dim)\n",
        "        action_dim=int(action_dim)\n",
        "        hidden_dim=int(hidden_dim)\n",
        "        # print(\"state_dim:\", state_dim)\n",
        "        # print(\"action_dim:\", action_dim)\n",
        "        # print(\"hidden_dim:\", hidden_dim)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)  # Outputs cost estimate\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"\n",
        "        Forward pass for the cost network.\n",
        "\n",
        "        Computes:\n",
        "        c_wv(s, a) = E[VaR(s, a)]  (Eq. 19 in the paper)\n",
        "\n",
        "        Args:\n",
        "        - state (torch.Tensor): State tensor with shape [batch_size, *]\n",
        "        - action (torch.Tensor): Action tensor with shape [batch_size, action_dim]\n",
        "\n",
        "        Returns:\n",
        "        - Cost estimation (torch.Tensor)\n",
        "        \"\"\"\n",
        "\n",
        "        # print(\" cost network forward ((((((((((((((((((((((((((((((((((((((()))))))))))))))))))))))))))))))))))))))\")\n",
        "        # 🔍 Print debug info to check tensor shapes\n",
        "        # print(\"state :: \", type(state) , state.shape)\n",
        "        # print(\"action :: \", type(action) , action.shape)\n",
        "        # 🔄 Flatten state if it has more than 2 dimensions\n",
        "        if state.dim() > 2:\n",
        "            state = state.view(state.shape[0], -1)  # Reshape to [batch_size, flattened_features]\n",
        "\n",
        "        # 🔄 Ensure action is 2D\n",
        "        if action.dim() > 2:\n",
        "            action = action.view(action.shape[0], -1)  # Reshape to [batch_size, action_dim]\n",
        "\n",
        "        # 🔍 Print final shapes\n",
        "        # print(f\"State shape after reshape: {state.shape}, Action shape after reshape: {action.shape}\")\n",
        "\n",
        "        # ✅ Now both state and action are 2D → Safe to concatenate\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        # Forward pass through the Cost network\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_gZvY6bQ94t"
      },
      "outputs": [],
      "source": [
        "#device = 'cpu'\n",
        "# Set the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIuoVt-8Q_xY",
        "outputId": "ca4e294c-fdfb-41d0-e5da-1fec53c6ced5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from scipy.stats import norm  # For z-score\n",
        "\n",
        "class DDPGagent:\n",
        "    def __init__(self, env, params, max_memory_size=50000):\n",
        "        \"\"\"\n",
        "        Initialize the DDPG agent with:\n",
        "        - Actor-Critic Networks\n",
        "        - Cost Network for risk constraints\n",
        "        - Target Networks for stability\n",
        "        - Lagrange multiplier for enforcing constraints\n",
        "        \"\"\"\n",
        "\n",
        "        # print(params)\n",
        "        print(\" DDPG AGEnt Class- ++++++++++++++++++++++++++++++++++++++++++\")\n",
        "\n",
        "        # 1️⃣ Define State & Action Space Dimensions\n",
        "        self.data = env.envs[0].df\n",
        "        curr_state= env.envs[0].state\n",
        "        # print(\"states_ ddpg init ::\", curr_state.shape)\n",
        "        actions = env.action_space.shape[0]\n",
        "\n",
        "        # print(\"actions ::\", actions)\n",
        "\n",
        "        self.num_states = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
        "        self.num_actions = env.action_space.shape[0]\n",
        "        self.gamma = params['gamma']  # Discount factor (γ)\n",
        "        self.tau = params['tau']  # Soft update factor (τ)\n",
        "        self.batch_size = int(params['batch_size'])\n",
        "        self.env = env\n",
        "\n",
        "        # 2️⃣ Initialize Networks\n",
        "        self.actor = Actor(self.num_states, self.num_actions, params['Ahidden_dim'],\n",
        "                           params['Anum_layers'], params['Aact_fn'], params['Adr']).to(device)\n",
        "        self.actor_target = Actor(self.num_states, self.num_actions, params['Ahidden_dim'],\n",
        "                                  params['Anum_layers'], params['Aact_fn'], params['Adr']).to(device)\n",
        "\n",
        "        self.critic = Critic(self.num_states, self.num_actions, params['Chidden_dim'],\n",
        "                             params['Cnum_layers'], params['Cact_fn'], params['Cdr']).to(device)\n",
        "        self.critic_target = Critic(self.num_states, self.num_actions, params['Chidden_dim'],\n",
        "                                    params['Cnum_layers'], params['Cact_fn'], params['Cdr']).to(device)\n",
        "\n",
        "        # 3️⃣ Initialize Cost Network for Constrained Reinforcement Learning\n",
        "        self.cost_network = CostNetwork(self.num_states, self.num_actions, params['Chidden_dim']).to(device)\n",
        "        self.cost_target = CostNetwork(self.num_states, self.num_actions, params['Chidden_dim']).to(device)\n",
        "\n",
        "        # Copy weights to target networks\n",
        "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "        for target_param, param in zip(self.cost_target.parameters(), self.cost_network.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "        # 4️⃣ Training Setup\n",
        "        self.memory = Memory(max_memory_size)\n",
        "        self.critic_criterion = nn.MSELoss()\n",
        "        self.cost_criterion = nn.MSELoss()\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=params['alr'])\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=params['clr'])\n",
        "        self.cost_optimizer = optim.Adam(self.cost_network.parameters(), lr=params['clr'])\n",
        "\n",
        "        # 5️⃣ Initialize Lagrange Multiplier for Constraint Enforcement\n",
        "        self.lambda_ = torch.tensor(0.01, requires_grad=False).to(device)\n",
        "        self.rho = 0.01  # Step size for updating lambda\n",
        "        self.violations= 0\n",
        "        self.zeta= env.envs[0].turbulence_threshold\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_tensor = torch.FloatTensor(state).to(device)\n",
        "        action = self.actor.forward(state_tensor).detach().cpu()\n",
        "\n",
        "        #action = action.detach().numpy()\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "    def VaR(self, states, actions, confidence_level=0.95):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        actions = actions.to(device)\n",
        "        states = states.to(device)  # assume actions is already on the correct device\n",
        "\n",
        "        batch_size = states.shape[0]  # ✅ Do NOT use `.to(device)` here\n",
        "        num_assets = 30\n",
        "\n",
        "        states = states.squeeze(1).to(device)  # [batch_size, 38, 30]\n",
        "        states_n = states  # already squeezed\n",
        "\n",
        "        cov_matrix = states[:, :num_assets, :].to(device)  # [batch_size, 30, 30]\n",
        "        hist_volatility = states_n[:, -1, :].to(device)  # [batch_size, 30]\n",
        "\n",
        "        z_score = torch.tensor(1.645, device=device)  # ✅ place tensor on the same device\n",
        "        individual_VaR = z_score * hist_volatility  # [batch_size, 30]\n",
        "\n",
        "        VaR_portfolio = torch.zeros(batch_size, device=device)  # ✅ directly initialize on device\n",
        "\n",
        "        for i in range(num_assets):\n",
        "            for j in range(num_assets):\n",
        "                VaR_portfolio = VaR_portfolio + (\n",
        "                    actions[:, i] * individual_VaR[:, i] *\n",
        "                    actions[:, j] * individual_VaR[:, j] * cov_matrix[:, i, j]\n",
        "                )\n",
        "\n",
        "        return VaR_portfolio\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_cost_target(self, states, actions, next_states, dones):\n",
        "        \"\"\"\n",
        "        Compute the target cost using the Bellman equation.\n",
        "\n",
        "        Equation (20):\n",
        "        c_{w_v}(s, a) = VaR(s, a) + \\eta (1 - d) c'_{w_v'}(s', a')\n",
        "        \"\"\"\n",
        "        next_actions = self.actor_target.forward(next_states)  # π'(s')\n",
        "        next_cost = self.cost_target.forward(next_states, next_actions.detach())  # c'_wv'(s', a')\n",
        "        cost_target = self.VaR(next_states, next_actions) + self.gamma * (1 - dones) * next_cost\n",
        "        return cost_target\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Perform one update step for the Actor, Critic, and Cost networks.\n",
        "        \"\"\"\n",
        "\n",
        "        # 1️⃣ Sample a batch from the Replay Buffer\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "\n",
        "        # print(\" update states : \", type(states),  states.shape,  \" action \", action.shape, type(action) )\n",
        "\n",
        "        # Remove the singleton dimension at dim=1\n",
        "        # states_n = states.squeeze(1)  # shape: (224, 38, 30)\n",
        "\n",
        "        # # Now slicing makes sense\n",
        "        # cov_mat = states_n[:, :30, :]                  # Shape: (224, 30, 30)\n",
        "        # histrical_volatility = states_n[:, -1, :]\n",
        "\n",
        "\n",
        "        # print(\"ddpg update - states_n ::\", states_n.shape)\n",
        "        # print(\"ddpg update - cov_mat_n ::\", cov_mat.shape)\n",
        "        # print(\"ddpg update - histrical_volatility_n ::\", histrical_volatility.shape)\n",
        "\n",
        "        # print(\"ddpg update - states_n ::\", states_n)\n",
        "        # print(\"ddpg update - cov_mat_n ::\", cov_mat)\n",
        "        # print(\"ddpg update - histrical_volatility_n ::\", histrical_volatility)\n",
        "\n",
        "\n",
        "        # next_states_n = next_states.squeeze(1)  # Shape: [batch_size, 38, 30]\n",
        "        # next_cov_mat = next_states_n[:, :30, :]  # Shape: [batch_size, 30, 30]\n",
        "        # next_hist_volatility = next_states_n[:, -1, :]\n",
        "        # print(\"ddpg update - next_states_n ::\", next_states_n.shape)\n",
        "        # print(\"ddpg update - next_cov_mat ::\", next_cov_mat.shape)\n",
        "        # print(\"ddpg update - histor vol :: \" , next_hist_volatility.shape)\n",
        "        # print(\"ddpg update - next_states_n ::\", next_states_n)\n",
        "        # print(\"ddpg update - next_cov_mat ::\", next_cov_mat)\n",
        "        # print(\"ddpg update - histor vol :: \" , next_hist_volatility)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "        # 4️⃣ Compute Target Q-Value using Bellman Equation (Eq. 5)\n",
        "        # Q(s, a) = r + γQ'(s', π'(s'))\n",
        "        Q_target = rewards + self.gamma * (1 - dones) * self.critic_target.forward(next_states, self.actor_target.forward(next_states).detach())\n",
        "\n",
        "        # 6️⃣ Compute Critic Loss (Eq. 6)\n",
        "        # L = 1/N \\sum (Q(s, a) - Q_target)^2\n",
        "        # print(\" critic loss calculation -start\")\n",
        "        critic_loss = self.critic_criterion(self.critic.forward(states, actions), Q_target.detach())\n",
        "\n",
        "        # print(\"critic loss calculation end \")\n",
        "        # 8️⃣ Compute Cost Network Loss (Eq. 21)\n",
        "        # L_C = 1/N \\sum (c_{w_v}(s, a) - VaR(s, a) - η (1 - d) c'_{w_v'}(s', a'))^2\n",
        "        # print(\"cost loss calculation started \")\n",
        "        cost_pred = self.cost_network.forward(states, actions)\n",
        "        cost_target = self.compute_cost_target(states, actions, next_states, dones).detach()\n",
        "        # print(\" cost_ target :: \" , cost_target)\n",
        "\n",
        "        cost_loss = self.cost_criterion(cost_pred, cost_target)\n",
        "\n",
        "        # print(\"cost loss calculation end \")\n",
        "        # 🔟 Compute Actor Loss using Lagrangian method (Eq. 13)\n",
        "        # L(w_π, λ) = -J_{w_π} + \\sum \\lambda_j C_{w_π, j} + \\frac{\\rho}{2} \\sum (C_{w_π, j})^2\n",
        "\n",
        "        # print(\"actor loss calculation started \")\n",
        "        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()\n",
        "        constraint_penalty =  cost_target\n",
        "\n",
        "        # print(\" constraint penalty before :: \", constraint_penalty)\n",
        "        # print(\" constraint_penalty :::: \" , constraint_penalty.shape, type(constraint_penalty))\n",
        "\n",
        "        violations_count = (constraint_penalty > self.zeta).sum().item()  # Count how many elements violate the constraint\n",
        "        # print(\" violations ::: \" , violations_count)\n",
        "        # Update the number of violations\n",
        "        self.violations  =  self.violations + violations_count\n",
        "\n",
        "\n",
        "        constraint_penalty = torch.where(\n",
        "            constraint_penalty <= self.zeta,\n",
        "            torch.tensor(0.0, device=constraint_penalty.device, dtype=constraint_penalty.dtype),\n",
        "            constraint_penalty - self.zeta\n",
        "        )\n",
        "        # print(\" constraint penalty after :: \", constraint_penalty)\n",
        "\n",
        "        quadratic_penalty = (self.rho / 2) * (self.cost_network.forward(states, actions) ** 2).mean().clone()\n",
        "        actor_loss = policy_loss + constraint_penalty + quadratic_penalty\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        # print(\" actor_ loss \",  actor_loss.shape)\n",
        "        actor_loss = actor_loss.mean()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "\n",
        "        # print(\"actor update end \")\n",
        "\n",
        "\n",
        "\n",
        "        # 1️⃣3️⃣ Soft Update of Target Networks (Eq. 14)\n",
        "        # print(\"soft update - critic -\")\n",
        "        with torch.no_grad():\n",
        "          for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "              target_param.data= param.data * self.tau + target_param.data * (1.0 - self.tau)\n",
        "\n",
        "          for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
        "              target_param.data = param.data * self.tau + target_param.data * (1.0 - self.tau)\n",
        "\n",
        "          for target_param, param in zip(self.cost_target.parameters(), self.cost_network.parameters()):\n",
        "              target_param.data = param.data * self.tau + target_param.data * (1.0 - self.tau)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # print(\" soft updates end\")\n",
        "\n",
        "    def buffer_fill(self, buffer_size):\n",
        "      state = self.env.reset()\n",
        "\n",
        "      # print(\" buffer fill ------ \")\n",
        "      # print(\" buffer fil state   --- \",  state.shape)\n",
        "      # print(\"  buffer fill state  --- \", state)\n",
        "      # print(\" buffer --- fill -- cov mat\", state[:, :30, :].shape)\n",
        "\n",
        "      # print(\" buffer fill -----hist vol\", state[:, -1, :].shape)\n",
        "      # print(\" buffer fill -----hist vol\", state[:, -1, :])\n",
        "\n",
        "      for _ in range(buffer_size):\n",
        "        action = self.get_action(state)\n",
        "        action = Noise(action, self.env.action_space)\n",
        "        new_state, reward, done, _ = self.env.step(action)\n",
        "        self.memory.push(state, action, reward, new_state, done)\n",
        "\n",
        "    def trade(self, val_env, e_val_gym):\n",
        "      Reward = []\n",
        "      state = val_env.reset()\n",
        "\n",
        "      for i in range(len(e_val_gym.df.index.unique())):\n",
        "        action = self.get_action(state)\n",
        "        next_obs, reward, done, _ = val_env.step(action.detach().numpy())\n",
        "        Reward.append(reward)\n",
        "\n",
        "        if i == (len(e_val_gym.df.index.unique()) - 2):\n",
        "          account_memory = val_env.env_method(method_name=\"save_asset_memory\")\n",
        "          actions_memory = val_env.env_method(method_name=\"save_action_memory\")\n",
        "\n",
        "        if done[0]:\n",
        "          print(\"hit end!\")\n",
        "          break\n",
        "        state = next_obs\n",
        "\n",
        "      return account_memory, actions_memory, sum(Reward)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M5fFJNoLRqA"
      },
      "outputs": [],
      "source": [
        "#Calculate the Sharpe ratio\n",
        "#This is our objective for tuning\n",
        "def calculate_sharpe(df):\n",
        "  #df['daily_return'] = df['account_value'].pct_change(1)\n",
        "  if df['daily_return'].std() !=0:\n",
        "    sharpe = (252**0.5)*df['daily_return'].mean()/ \\\n",
        "          df['daily_return'].std()\n",
        "    return sharpe\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY-eM8HWZW4o"
      },
      "outputs": [],
      "source": [
        "space = {\n",
        "    'Ahidden_dim': hp.quniform('Ahidden_dim', 2, 512, 1),\n",
        "    'Anum_layers': hp.quniform('Anum_layers', 1, 8, 1),\n",
        "    'Chidden_dim': hp.quniform('Chidden_dim', 2, 512, 1),\n",
        "    'Cnum_layers': hp.quniform('Cnum_layers', 1, 8, 1),\n",
        "\n",
        "    'alr': hp.loguniform('alr', -8, -1),  # Actor learning rate\n",
        "    'clr': hp.loguniform('clr', -8, -1),  # Critic learning rate\n",
        "    'gamma': hp.uniform('gamma', 0.9, 0.99),  # Discount factor\n",
        "    'tau': hp.uniform('tau', 0.08, 0.2),  # Soft target update rate\n",
        "    'batch_size': hp.quniform('batch_size', 32, 256, 32),  # Mini-batch size\n",
        "\n",
        "    'Aact_fn': hp.choice('Aact_fn', ['relu', 'tanh', 'sigmoid']),  # Actor activation\n",
        "    'Adr': hp.uniform('Adr', 0, 0.5),  # Actor dropout\n",
        "    'Cact_fn': hp.choice('Cact_fn', ['relu', 'tanh', 'sigmoid']),  # Critic activation\n",
        "    'Cdr': hp.uniform('Cdr', 0, 0.5),  # Critic dropout\n",
        "\n",
        "    # 🚀 **Newly Added Missing Hyperparameters**:\n",
        "    'rho': hp.uniform('rho', 0.001, 0.1),  # Lagrange multiplier update step size\n",
        "    # 'lambda_init': hp.uniform('lambda_init', 0.01, 1.0),  # Initial value of λ\n",
        "    'buffer_size': hp.quniform('buffer_size', 10000, 1000000, 10000),  # Replay buffer size\n",
        "    'noise_std': hp.uniform('noise_std', 0.01, 0.3),  # Exploration noise level\n",
        "    'grad_clip': hp.uniform('grad_clip', 0.1, 10.0),  # Gradient clipping threshold\n",
        "    'warmup_steps': hp.quniform('warmup_steps', 1000, 50000, 1000),  # Steps before training starts\n",
        "    'reward_scaling': hp.uniform('reward_scaling', 0.1, 10.0)  # Reward scaling factor\n",
        "}\n",
        "\n",
        "\n",
        "def objective(params):\n",
        "    print(params)\n",
        "    # Convert hyperparameters to integers where necessary\n",
        "    params['Ahidden_dim'] = int(params['Ahidden_dim'])\n",
        "    params['Anum_layers'] = int(params['Anum_layers'])\n",
        "    params['Chidden_dim'] = int(params['Chidden_dim'])\n",
        "    params['Cnum_layers'] = int(params['Cnum_layers'])\n",
        "    params['batch_size'] = int(params['batch_size'])\n",
        "    params['buffer_size'] = int(params['buffer_size'])\n",
        "    params['warmup_steps'] = int(params['warmup_steps'])\n",
        "\n",
        "    model = DDPGagent(env_train, params)\n",
        "    model.buffer_fill(500)\n",
        "    model.update()\n",
        "\n",
        "    account_memory, actions_memory, rewardd = model.trade(env_val, e_val_gym)\n",
        "    print( f\" the reward is :::::::    {rewardd}  \" )\n",
        "\n",
        "    sharpe = calculate_sharpe(account_memory[0])\n",
        "    return -sharpe\n",
        "    # return -reward[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC1UCl-g7GZk",
        "outputId": "7e67f112-9208-4e85-c0cd-9d80c318602d"
      },
      "outputs": [],
      "source": [
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals= 10 , trials=Trials()) #max_evals = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjCWHqYazkF2",
        "outputId": "6e1cc772-5e46-405d-e4c7-5235a262f083"
      },
      "outputs": [],
      "source": [
        "best['Aact_fn'] = ['relu', 'tanh', 'sigmoid'][best['Aact_fn']]\n",
        "best['Cact_fn'] = ['relu', 'tanh', 'sigmoid'][best['Cact_fn']]\n",
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5RHcOfsBALp"
      },
      "outputs": [],
      "source": [
        "# best= {'Aact_fn': 'sigmoid',\n",
        "#  'Adr': np.float64(0.40156739994186963),\n",
        "#  'Ahidden_dim': np.float64(320.0),\n",
        "#  'Anum_layers': np.float64(2.0),\n",
        "#  'Cact_fn': 'tanh',\n",
        "#  'Cdr': np.float64(0.1254413062439358),\n",
        "#  'Chidden_dim': np.float64(251.0),\n",
        "#  'Cnum_layers': np.float64(5.0),\n",
        "#  'alr': np.float64(0.0009107119585481395),\n",
        "#  'batch_size': np.float64(128.0),\n",
        "#  'buffer_size': np.float64(750000.0),\n",
        "#  'clr': np.float64(0.011909541457606108),\n",
        "#  'gamma': np.float64(0.9802858202890133),\n",
        "#  'grad_clip': np.float64(4.985696483611227),\n",
        "#  'lambda_init': np.float64(0.4925723364653761),\n",
        "#  'noise_std': np.float64(0.23649933117828195),\n",
        "#  'reward_scaling': np.float64(4.922508316091725),\n",
        "#  'rho': np.float64(0.0701825073670665),\n",
        "#  'tau': np.float64(0.1185136176470432),\n",
        "#  'warmup_steps': np.float64(9000.0)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4QHUv8qBDGP",
        "outputId": "f611f325-e23e-49b9-f0a9-83364ff51e1f"
      },
      "outputs": [],
      "source": [
        "agent = DDPGagent(env_full_train, best)\n",
        "\n",
        "batch_size = agent.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sySnaXAOBExQ",
        "outputId": "0c6299cc-535c-4f28-c967-9ce68a8d63fe"
      },
      "outputs": [],
      "source": [
        "rewards = []\n",
        "avg_rewards = []\n",
        "num_episodes = 5 #1000\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "    state = env_full_train.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "    # print(state.shape, type(state))\n",
        "    # state = state.reshape(1, 1, 39, 30)\n",
        "    # print((torch.tensor( np.expand_dims(state, axis=1))).dim)\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Episode: {episode+1}\")\n",
        "    while not done:\n",
        "\n",
        "        # print(i)\n",
        "        # print(f\"done  : {done} \")\n",
        "        action = agent.get_action(state)\n",
        "        action = Noise(action, env_full_train.action_space)\n",
        "        new_state, reward, done ,info = env_full_train.step(action)\n",
        "        # done= terminated or truncated\n",
        "        agent.memory.push(state, action, reward, new_state, done)\n",
        "\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.update()\n",
        "\n",
        "        state = new_state\n",
        "        episode_reward  = episode_reward + reward\n",
        "\n",
        "        if done:\n",
        "          #  sys.stdout.write(\"episode: {}, reward: {}, average _reward: {} \\n\".format(episode, np.round(episode_reward, decimals=2), np.mean(rewards[-10:])))\n",
        "            break\n",
        "\n",
        "    # agent.lambda_ = agent.lambda_ + agent.rho * agent.cost_network.forward(torch.tensor( np.expand_dims(state, axis=1)), agent.get_action(state)).mean()\n",
        "    # agent.lambda_ = agent.lambda_ + agent.rho * agent.cost_network.forward(\n",
        "                  #     torch.tensor(np.expand_dims(state, axis=1), dtype=torch.float32),\n",
        "                  #     agent.get_action(state)\n",
        "                  # ).mean().detach()\n",
        "\n",
        "    device = next(agent.cost_network.parameters()).device  # Get the device of the cost network\n",
        "\n",
        "    state_tensor = torch.tensor(np.expand_dims(state, axis=1), dtype=torch.float32, device=device)\n",
        "    action_tensor = agent.get_action(state).to(device)  # Ensure action is also on same device\n",
        "\n",
        "    agent.lambda_ = agent.lambda_ + agent.rho * agent.cost_network.forward(\n",
        "        state_tensor,\n",
        "        action_tensor\n",
        "    ).mean().detach().to(device)\n",
        "\n",
        "    agent.rho= agent.rho * 1.008\n",
        "\n",
        "    rewards.append(episode_reward)\n",
        "    avg_rewards.append(np.mean(rewards[-10:]))\n",
        "    print(f\"Episode: {episode+1}, Total Reward: {episode_reward}\")\n",
        "    print(\" violations : \" ,  agent.violations)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luSBnlTubh4w"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "# Save to file\n",
        "with open(\"ddpg_agent.pkl\", \"wb\") as f:\n",
        "    pickle.dump(agent, f)\n",
        "\n",
        "files.download(\"ddpg_agent.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14v3agTOBaxy"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot the rewards\n",
        "ax.plot(rewards, label='Rewards')\n",
        "ax.plot(avg_rewards, label='Average Rewards')\n",
        "\n",
        "# Label the axes\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Reward')\n",
        "\n",
        "# Add legend\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "fig.savefig('rewards_plot.png')\n",
        "files.download(\"rewards_plot.png\")\n",
        "\n",
        "# Now `fig` contains the plot and can be saved or manipulated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Ma6YpTlnuZ"
      },
      "source": [
        "## Trading\n",
        "Assume that we have $1,000,000 initial capital at 2019-01-01. We use the A2C model to trade Dow jones 30 stocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfibsz_CU7-k"
      },
      "source": [
        "import the trading dataframe  and hist_vol dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfF4uXy9VDsc"
      },
      "outputs": [],
      "source": [
        "indices= [sensex_ticker, Dow_30, dax_30, nikkei_top30_symbols, FTSE_top30, twse_top30, hang_seng_symbols, brazil_tickers, ibex35_tickers, bist100_top30_tickers ]\n",
        "\n",
        "# indices = {\n",
        "#     \"sensex_ticker\": sensex_ticker,\n",
        "#     \"Dow_30\": Dow_30,\n",
        "#     \"dax_30\": dax_30,\n",
        "#     \"nikkei_top30_symbols\": nikkei_top30_symbols,\n",
        "#     \"FTSE_top30\": FTSE_top30,\n",
        "#     \"twse_top30\": twse_top30,\n",
        "#     \"hang_seng_symbols\": hang_seng_symbols,\n",
        "#     \"brazil_tickers\": brazil_tickers,\n",
        "#     \"ibex_35_tickers\": ibex35_tickers,\n",
        "#     \"bist100_top30_tickers\": bist100_top30_tickers\n",
        "# }\n",
        "\n",
        "file_name= \"____index that u  want to  trade into __ \"\n",
        "\n",
        "df= pd.read_csv(f'/content/stock_data_{file_name}')\n",
        "hist_vol_trade = pd.read_csv(f'/content/hist_vol_{file_name}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaWbMO8DBlJU"
      },
      "outputs": [],
      "source": [
        "trade = data_split(df,'2023-01-01', '2025-02-28')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pO-A2ycU236"
      },
      "outputs": [],
      "source": [
        "TURBULENCE_THRESHOLD\n",
        "env_kwargs_trade = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"transaction_cost_pct\": 0.001,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"hist_vol\":hist_vol_trade,\n",
        "    \"turbulence_threshold\": TURBULENCE_THRESHOLD\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu8e4T4TBkOj"
      },
      "outputs": [],
      "source": [
        "e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs_trade)\n",
        "test_env, test_obs = e_trade_gym.get_sb_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoZQ9QSkBkHw"
      },
      "outputs": [],
      "source": [
        "account_memory, actions_memory, rewardd = agent.trade(env_trade, e_trade_gym)\n",
        "violations= agent.violations\n",
        "\n",
        "print(\"violations : \" , violations)\n",
        "print(\" reward :: \" , rewardd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNWkmXbPBn7Y"
      },
      "outputs": [],
      "source": [
        "calculate_sharpe(account_memory[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc_7ZgJxBo7T"
      },
      "outputs": [],
      "source": [
        "account_memory[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOVj9UrOBp0H"
      },
      "outputs": [],
      "source": [
        "account_memory[0].to_csv('/content/df_daily_return.csv')\n",
        "files.download('df_daily_return.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezqc3qrTBrMX"
      },
      "outputs": [],
      "source": [
        "actions_memory[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wovLmqXHBrA6"
      },
      "outputs": [],
      "source": [
        "actions_memory[0].to_csv('/content/df_actions.csv')\n",
        "files.download('df_actions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzvTQ_mrBt_v"
      },
      "outputs": [],
      "source": [
        "df_daily_return = account_memory[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFO42LcomPUT"
      },
      "source": [
        "<a id='6'></a>\n",
        "# Part 7: Backtest Our Strategy\n",
        "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y1DIsSgwqUK"
      },
      "outputs": [],
      "source": [
        "#calculate_portfolio_minimum_variance\n",
        "portfolio = pd.DataFrame(index = range(1), columns = unique_trade_date)\n",
        "initial_capital = 1000000\n",
        "portfolio.loc[0,unique_trade_date[0]] = initial_capital\n",
        "\n",
        "# Define transaction cost rate\n",
        "transaction_cost_rate = 0.005\n",
        "\n",
        "for i in range(len( unique_trade_date)-1):\n",
        "    df_temp = df[df.date==unique_trade_date[i]].reset_index(drop=True)\n",
        "    df_temp_next = df[df.date==unique_trade_date[i+1]].reset_index(drop=True)\n",
        "    #Sigma = risk_models.sample_cov(df_temp.return_list[0])\n",
        "    #calculate covariance matrix\n",
        "    Sigma = df_temp.return_list[0].cov()\n",
        "    #portfolio allocation\n",
        "    ef_min_var = EfficientFrontier(None, Sigma,weight_bounds=(0, 0.1))\n",
        "    #minimum variance\n",
        "    raw_weights_min_var = ef_min_var.min_volatility()\n",
        "    #get weights\n",
        "    cleaned_weights_min_var = ef_min_var.clean_weights()\n",
        "\n",
        "    #current capital\n",
        "    cap = portfolio.iloc[0, i]\n",
        "    #current cash invested for each stock\n",
        "    current_cash = [element * cap for element in list(cleaned_weights_min_var.values())]\n",
        "    # current held shares\n",
        "    current_shares = list(np.array(current_cash)\n",
        "                                      / np.array(df_temp.close))\n",
        "    # next time period price\n",
        "    next_price = np.array(df_temp_next.close)\n",
        "\n",
        "    # Calculate next portfolio value without transaction cost\n",
        "    next_value = np.dot(current_shares, next_price)\n",
        "\n",
        "    # Calculate transaction costs\n",
        "    new_shares = current_cash / next_price\n",
        "    share_differences = np.abs(new_shares - current_shares)\n",
        "    transaction_cost = np.sum(share_differences * next_price * transaction_cost_rate)\n",
        "\n",
        "    # Deduct transaction cost from portfolio value\n",
        "    portfolio.iloc[0, i + 1] = next_value - transaction_cost\n",
        "\n",
        "portfolio=portfolio.T\n",
        "portfolio.columns = ['account_value']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtFE2lK7wsY-"
      },
      "outputs": [],
      "source": [
        "def calculate_daily_return(current_value, previous_value):\n",
        "    return (current_value - previous_value) / previous_value\n",
        "\n",
        "# Calculate daily return and add it as a new column\n",
        "daily_returns = [0]  # Daily return for the first day is assumed to be 0\n",
        "for i in range(1, len(portfolio)):\n",
        "    current_value = portfolio['account_value'][i]\n",
        "    previous_value = portfolio['account_value'][i - 1]\n",
        "    daily_returns.append(calculate_daily_return(current_value, previous_value))\n",
        "\n",
        "portfolio['daily_return'] = daily_returns\n",
        "\n",
        "print(portfolio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeL1RZ1wwva6"
      },
      "outputs": [],
      "source": [
        "portfolio.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dshrV1ZHTt"
      },
      "outputs": [],
      "source": [
        "Agent =(df_daily_return_T.daily_return+1).cumprod()-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNa6ocUSZIwJ"
      },
      "outputs": [],
      "source": [
        "min_var_cumpod =(portfolio.account_value.pct_change()+1).cumprod()-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Slu8yVVYZKBs"
      },
      "outputs": [],
      "source": [
        "portfolio.drop(columns=['account_value'], inplace=True)\n",
        "portfolio.to_csv('Markowitz_Portfolio_Return_'+ Market +'.csv')\n",
        "files.download('Markowitz_Portfolio_Return_'+ Market +'.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st6NDDuxZLbp"
      },
      "outputs": [],
      "source": [
        "Baseline =(baseline_returns+1).cumprod()-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmCGCfoaZPog"
      },
      "source": [
        "## Plotly: DRL, Min-Variance, DJIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G18-cwvVZRof"
      },
      "outputs": [],
      "source": [
        "%pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqFhJ0peZSvG"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime as dt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLoB5e7RZTwz"
      },
      "outputs": [],
      "source": [
        "time_ind = pd.Series(df_daily_return_T.date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw1HTKMfZU0C"
      },
      "outputs": [],
      "source": [
        "trace0_portfolio = go.Scatter(x = time_ind, y = Agent, mode = 'lines', name = 'Agent (Portfolio Allocation)')\n",
        "\n",
        "trace1_portfolio = go.Scatter(x = time_ind, y = Baseline, mode = 'lines', name = 'Baseline')\n",
        "trace2_portfolio = go.Scatter(x = time_ind, y = min_var_cumpod, mode = 'lines', name = 'Min-Variance')\n",
        "#trace3_portfolio = go.Scatter(x = time_ind, y = a2c_cumpod_esg, mode = 'lines', name = 'ESG-A2C (Portfolio Allocation)')\n",
        "#trace3_portfolio = go.Scatter(x = time_ind, y = ddpg_cumpod, mode = 'lines', name = 'DDPG')\n",
        "#trace4_portfolio = go.Scatter(x = time_ind, y = addpg_cumpod, mode = 'lines', name = 'Adaptive-DDPG')\n",
        "#trace5_portfolio = go.Scatter(x = time_ind, y = min_cumpod, mode = 'lines', name = 'Min-Variance')\n",
        "\n",
        "#trace4 = go.Scatter(x = time_ind, y = addpg_cumpod, mode = 'lines', name = 'Adaptive-DDPG')\n",
        "\n",
        "#trace2 = go.Scatter(x = time_ind, y = portfolio_cost_minv, mode = 'lines', name = 'Min-Variance')\n",
        "#trace3 = go.Scatter(x = time_ind, y = spx_value, mode = 'lines', name = 'SPX')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AswRB2OrZfZ-"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(trace0_portfolio)\n",
        "\n",
        "fig.add_trace(trace1_portfolio)\n",
        "\n",
        "fig.add_trace(trace2_portfolio)\n",
        "\n",
        "#fig.add_trace(trace3_portfolio)\n",
        "\n",
        "fig.update_layout(\n",
        "    legend=dict(\n",
        "        x=0,\n",
        "        y=1,\n",
        "        traceorder=\"normal\",\n",
        "        font=dict(\n",
        "            family=\"sans-serif\",\n",
        "            size=15,\n",
        "            color=\"black\"\n",
        "        ),\n",
        "        bgcolor=\"White\",\n",
        "        bordercolor=\"white\",\n",
        "        borderwidth=2\n",
        "\n",
        "    ),\n",
        ")\n",
        "#fig.update_layout(legend_orientation=\"h\")\n",
        "fig.update_layout(title={\n",
        "        #'text': \"Cumulative Return using FinRL\",\n",
        "        'y':0.85,\n",
        "        'x':0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'})\n",
        "#with Transaction cost\n",
        "#fig.update_layout(title =  'Quarterly Trade Date')\n",
        "fig.update_layout(\n",
        "#    margin=dict(l=20, r=20, t=20, b=20),\n",
        "\n",
        "    paper_bgcolor='rgba(1,1,0,0)',\n",
        "    plot_bgcolor='rgba(1, 1, 0, 0)',\n",
        "    #xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Cumulative Return\",\n",
        "xaxis={'type': 'date',\n",
        "       'tick0': time_ind[0],\n",
        "        'tickmode': 'linear',\n",
        "       'dtick': 86400000.0 *80}\n",
        "\n",
        ")\n",
        "fig.update_xaxes(showline=True,linecolor='black',showgrid=True, gridwidth=1, gridcolor='LightSteelBlue',mirror=True)\n",
        "fig.update_yaxes(showline=True,linecolor='black',showgrid=True, gridwidth=1, gridcolor='LightSteelBlue',mirror=True)\n",
        "fig.update_yaxes(zeroline=True, zerolinewidth=1, zerolinecolor='LightSteelBlue')\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pR_IEVb9CYtB",
        "S35Ryj9ZCmKp"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
